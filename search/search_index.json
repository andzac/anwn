{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Homepage Dear visitor, this is my personal web-notes. It will be used by me to clip all the useful notes about programming language and more in general what can be useful to save in a persistent way. I hope you can found something useful for you, have a nice day!","title":"Homepage"},{"location":"#homepage","text":"Dear visitor, this is my personal web-notes. It will be used by me to clip all the useful notes about programming language and more in general what can be useful to save in a persistent way. I hope you can found something useful for you, have a nice day!","title":"Homepage"},{"location":"about/","text":"Hi, we're work in progress","title":"About"},{"location":"about/#hi-were-work-in-progress","text":"","title":"Hi, we're work in progress"},{"location":"Czech Lessons/accent/","text":"Alphabet A, \u00c1, B, C, \u010c, D, \u010e, E, \u00c9, \u011a, F, G, H, Ch, I, \u00cd, J, K, L, M, N, \u0147, O, \u00d3, P, Q, R, \u0158, S, \u0160, T, \u0164, U, \u00da, \u016e, V, W, X, Y, \u00dd, Z, \u017d Keyboard shortcut Char ASCII code \u00e1 ALT+160 \u00e9 ALT+130 \u00ed ALT+161 \u00f3 ALT+162 \u00da ALT+163 \u0158, \u0159 ALT+268, ALT+269 \u010c, \u010d ALT+344, ALT+345 \u017d, \u017e ALT+381, ALT+382","title":"Alphabet"},{"location":"Czech Lessons/accent/#alphabet","text":"A, \u00c1, B, C, \u010c, D, \u010e, E, \u00c9, \u011a, F, G, H, Ch, I, \u00cd, J, K, L, M, N, \u0147, O, \u00d3, P, Q, R, \u0158, S, \u0160, T, \u0164, U, \u00da, \u016e, V, W, X, Y, \u00dd, Z, \u017d","title":"Alphabet"},{"location":"Czech Lessons/accent/#keyboard-shortcut","text":"Char ASCII code \u00e1 ALT+160 \u00e9 ALT+130 \u00ed ALT+161 \u00f3 ALT+162 \u00da ALT+163 \u0158, \u0159 ALT+268, ALT+269 \u010c, \u010d ALT+344, ALT+345 \u017d, \u017e ALT+381, ALT+382","title":"Keyboard shortcut"},{"location":"Czech Lessons/pronouns/","text":"Personal pronouns EN CZ I J\u00e1 You ty he/she/it on/ona/ono we my you vy they oni/ony/ona","title":"Personal pronouns"},{"location":"Czech Lessons/pronouns/#personal-pronouns","text":"EN CZ I J\u00e1 You ty he/she/it on/ona/ono we my you vy they oni/ony/ona","title":"Personal pronouns"},{"location":"Czech Lessons/ten_activities/","text":"10 Activities in Czech 1. 2. ... 3. ... 4. ... 5. ... 6. ... 7. ... 8. ... 9. ... 10. ...","title":"Ten activities"},{"location":"Czech Lessons/ten_activities/#10-activities-in-czech","text":"1. 2. ... 3. ... 4. ... 5. ... 6. ... 7. ... 8. ... 9. ... 10. ...","title":"10 Activities in Czech"},{"location":"Development docs/Docker/UsefulCommand/","text":"Most used Docker commands build an image using a Dockerfile docker build - < Dockerfile -t tag-name delete all containers: docker rm $(docker ps -a -q) delete all images: docker rmi $(docker images -q)","title":"Most used Docker commands"},{"location":"Development docs/Docker/UsefulCommand/#most-used-docker-commands","text":"build an image using a Dockerfile docker build - < Dockerfile -t tag-name delete all containers: docker rm $(docker ps -a -q) delete all images: docker rmi $(docker images -q)","title":"Most used Docker commands"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/","text":"Clustering In basis it is required to manage docker container running on multiple hosts in a distributed enviroment it helps to scale docker container effectively solution for enterprice there is a manager host and more agent on other host that will join in the Cluster each software or implementation of the clustering shouldn't violate the docker client principles, it will be possible using CLI to perform the basic docker command in the cluster. when is necessary realize a cluster (or perform scaling ops) is necessary keep in mind some prerequisite that linux OS needs, as right permission, volume handling and so on Using docker swarm for clustering Grab the latest version with the command: docker pull swarm create a simple swarm docker run --rm swarm create This process will return the token id that will be used by the other nodes that will be involving in the cluster. It is unique identifier in the cluster and it could be used also to grab information about the cluster (how many nodes are defined into the cluster and so on). NOTE When is necessary use the token (to discover services for example) in necessary use only the first 5 digits. start to create a new node into the created cluster wuth the command sudo docker -H tcp://<host_id>:<port_id> -d & So now the new node with the IP above will be the default socket and the daemon will try to create the node in the cluster and waiting for connection. The complexity of this operation is that is necessary manage the docker socket and perform some dangerous operation, that can be skipped simply using docker swarm NOTE Before create the cluster there are a list of checks that need to be reviewed before to create the swarm as example verify that TLS is enabled and other stuff. see docker checks Obtain list of nodes in the cluster Using the token id is very simple retreive the nodes defined in the cluster: docker run --rm swarm list token://<token_id> Discovery services There are some algorithmics that allow a node to join in a exsisting cluster. In order to discover a cluster for the defined node is possible invoke the command: docker run swarm join --advertise=<host_id>:<port_id> token://<token_id> Is so possible verify if the node master with the given id is ready to manage other nodes that should be added in the cluster in according with this reference. Using filters with docker swarm Filters are used to schedule containers on a subset of nodes they are an important part of the docker swarm manager command there are several filters flag that can be used along with the swarm command to manage a particular kind of nodes. some example: deploy a container on nodes that have ssd as storage drive, or are located on the east coast of US, or are production nodes, and so on. NOTE if more than one node are selected during the filter operation, docker manage will select one random nodes from these result list. Filter types: constraint key/value associated to a particular nodes ```sh docker run -d -P -e constraint:storage=ssd --name test nginx ``` affinity attach container based on a given label, id or name we can instruct docker to run a container in a node and the next in another specified node. This could allow that the containers are all in the same network. Example Spawn the first container (called frontend) as usual: docker run -d -p 80:80 --name nginx_server nginx Generate the next container defining an affinity with the previous container created: sh docker run -d --name second_container -e affinity:container==nginx_server nginx port node selection based on a specific port i.e. spawn containers that have the port 80 exposed. Of course these containers are defined on multiple host in the cluster docker run -d -p 80:80 --name nginx_server nginx NOTE if we try to run this command more than one time on a single node, at the second time docker reply with an error message because on the single node there are already defined a container that use the port 80. But if we try this command on a cluster with several nodes, the port filters allow us that the docker swarm engine will checks if there is another node in the cluster that have the 80 available and is not already occupied by a container. dependency node selection based on a given dependent volume or link docker run -d -P --link=dependency:db1 --name db2 nginx Here we assume there is an exsisting container called db1, and docker swarm engine will try to spawn the new container db2 on the same node where are defined db1 because db2 have a dependency from db1. NOTE if docker can't resolve the dependency (for example because the db1 container doesn't exsist), the db2 creation is stopped and the container is not created. health scheduling containers on unhealthy nodes standard filters provided by docker itself out of the box (node ID/node name, storagedriver, kernelversion and so on) Running a container using a filter For example I want launch nginx container on nodes of a given cluster that have ssd storage drive: NOTE -d flag is for launch the container as daemon -P flag is for the port -e for the constraint (with the assumption that exsist a label or property for the constraint define)\u00f9 -m flat to associate an amount of memory Swarm strategies Swarming on a cluster is a complex operation under the hood, but from the docker prospective is just an API that could be used to define our infrastructure Docker swarm support internally multiple strategy to allocate containers that are partecipating in a cluster. Strategies are algorithmics that determine the score to rank a node in a cluster Depending on the strategy applied a container can be defined on a node or on a different one. The container will be defined on the node with the maximum score, based on a given strategy. The flag --swarm-strategy < binpack | random | spread > should be used when the node is created in the cluster Actually there are three types of strategies: BinPack: favor nodes that are running the maximum number of containers. this strategy allow to use at max as possible the space/memory capacity of a node before create or use another one Random: is more simple but could became very dangerous if the selected container is full could be not used in production enviroment Spread: it allow to select only the nodes with the minumum number of running containers. This could be a good strategy to minimize the effort necessary to restart containers on a node that for some reasons crash.","title":"Clustering"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#clustering","text":"In basis it is required to manage docker container running on multiple hosts in a distributed enviroment it helps to scale docker container effectively solution for enterprice there is a manager host and more agent on other host that will join in the Cluster each software or implementation of the clustering shouldn't violate the docker client principles, it will be possible using CLI to perform the basic docker command in the cluster. when is necessary realize a cluster (or perform scaling ops) is necessary keep in mind some prerequisite that linux OS needs, as right permission, volume handling and so on","title":"Clustering"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#using-docker-swarm-for-clustering","text":"Grab the latest version with the command: docker pull swarm create a simple swarm docker run --rm swarm create This process will return the token id that will be used by the other nodes that will be involving in the cluster. It is unique identifier in the cluster and it could be used also to grab information about the cluster (how many nodes are defined into the cluster and so on).","title":"Using docker swarm for clustering"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#note","text":"When is necessary use the token (to discover services for example) in necessary use only the first 5 digits. start to create a new node into the created cluster wuth the command sudo docker -H tcp://<host_id>:<port_id> -d & So now the new node with the IP above will be the default socket and the daemon will try to create the node in the cluster and waiting for connection. The complexity of this operation is that is necessary manage the docker socket and perform some dangerous operation, that can be skipped simply using docker swarm","title":"NOTE"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#note_1","text":"Before create the cluster there are a list of checks that need to be reviewed before to create the swarm as example verify that TLS is enabled and other stuff. see docker checks","title":"NOTE"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#obtain-list-of-nodes-in-the-cluster","text":"Using the token id is very simple retreive the nodes defined in the cluster: docker run --rm swarm list token://<token_id>","title":"Obtain list of nodes in the cluster"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#discovery-services","text":"There are some algorithmics that allow a node to join in a exsisting cluster. In order to discover a cluster for the defined node is possible invoke the command: docker run swarm join --advertise=<host_id>:<port_id> token://<token_id> Is so possible verify if the node master with the given id is ready to manage other nodes that should be added in the cluster in according with this reference.","title":"Discovery services"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#using-filters-with-docker-swarm","text":"Filters are used to schedule containers on a subset of nodes they are an important part of the docker swarm manager command there are several filters flag that can be used along with the swarm command to manage a particular kind of nodes. some example: deploy a container on nodes that have ssd as storage drive, or are located on the east coast of US, or are production nodes, and so on.","title":"Using filters with docker swarm"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#note_2","text":"if more than one node are selected during the filter operation, docker manage will select one random nodes from these result list.","title":"NOTE"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#filter-types","text":"constraint key/value associated to a particular nodes ```sh docker run -d -P -e constraint:storage=ssd --name test nginx ``` affinity attach container based on a given label, id or name we can instruct docker to run a container in a node and the next in another specified node. This could allow that the containers are all in the same network.","title":"Filter types:"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#example","text":"Spawn the first container (called frontend) as usual: docker run -d -p 80:80 --name nginx_server nginx Generate the next container defining an affinity with the previous container created: sh docker run -d --name second_container -e affinity:container==nginx_server nginx port node selection based on a specific port i.e. spawn containers that have the port 80 exposed. Of course these containers are defined on multiple host in the cluster docker run -d -p 80:80 --name nginx_server nginx","title":"Example"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#note_3","text":"if we try to run this command more than one time on a single node, at the second time docker reply with an error message because on the single node there are already defined a container that use the port 80. But if we try this command on a cluster with several nodes, the port filters allow us that the docker swarm engine will checks if there is another node in the cluster that have the 80 available and is not already occupied by a container. dependency node selection based on a given dependent volume or link docker run -d -P --link=dependency:db1 --name db2 nginx Here we assume there is an exsisting container called db1, and docker swarm engine will try to spawn the new container db2 on the same node where are defined db1 because db2 have a dependency from db1.","title":"NOTE"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#note_4","text":"if docker can't resolve the dependency (for example because the db1 container doesn't exsist), the db2 creation is stopped and the container is not created. health scheduling containers on unhealthy nodes standard filters provided by docker itself out of the box (node ID/node name, storagedriver, kernelversion and so on)","title":"NOTE"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#running-a-container-using-a-filter","text":"For example I want launch nginx container on nodes of a given cluster that have ssd storage drive:","title":"Running a container using a filter"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#note_5","text":"-d flag is for launch the container as daemon -P flag is for the port -e for the constraint (with the assumption that exsist a label or property for the constraint define)\u00f9 -m flat to associate an amount of memory","title":"NOTE"},{"location":"Development docs/Docker/Clustering&Swarm/dockerSwarm/#swarm-strategies","text":"Swarming on a cluster is a complex operation under the hood, but from the docker prospective is just an API that could be used to define our infrastructure Docker swarm support internally multiple strategy to allocate containers that are partecipating in a cluster. Strategies are algorithmics that determine the score to rank a node in a cluster Depending on the strategy applied a container can be defined on a node or on a different one. The container will be defined on the node with the maximum score, based on a given strategy. The flag --swarm-strategy < binpack | random | spread > should be used when the node is created in the cluster Actually there are three types of strategies: BinPack: favor nodes that are running the maximum number of containers. this strategy allow to use at max as possible the space/memory capacity of a node before create or use another one Random: is more simple but could became very dangerous if the selected container is full could be not used in production enviroment Spread: it allow to select only the nodes with the minumum number of running containers. This could be a good strategy to minimize the effort necessary to restart containers on a node that for some reasons crash.","title":"Swarm strategies"},{"location":"Development docs/Docker/Docker compose/dockerCompose/","text":"-------- docker compose ----------- docker compose s a hand tool to run multicontainer application wi Docker multicontainer app on a single node! --> how to spawn images orchestrate a set of container minimizing the configuration of each container on each host and the effort to launch each separately (i don't need to define properties for runtime for each single container) hi level configuration defined in a file structure -definition of spawing such multicontainer applications are contained in a single file or couple of fole (one to specify what those containers are and one to binding the requirement) -it is a great tool for dvlp and stagineg enviroment Use docker compose (3 steps) 1) create docker file --> image to use, set env variables, directories that should contain the codebase 2) define services required in the docker-compose.yml file --> volume, ports, rights, directorie, bindings 3) run docker-compose up to spin the applciation yaml file are not markup language - it is a human readable definition and it is becoming significant important to define script languages - docker compose help to manage the entire lifecycle of an application - manage the multichannel enviroment that we desire to spawn - start, stop and rebuild services - view status of running services - stream the log output of running services - run a one-off cmd on a services docker compose spawn application only within the context of a single host --> is not possible perform clustering with compose --> context should be a single host docker compose is perfect for pre-production enviroment where testing is performed on a single node, but in production we need to manage clustering multiple host and multiple container install docker compose on ubuntu -not support windows os - docker installation and engine on required - grant permission for write/read ops --> best be root (cmd: su root) --> otherwise I need to chmod +x on the docker compose folder 1) grab the docker compose from github 2) using the pip utility python based utility --> pip install -U docker-compose (at first time is necessary update the system - do it automatically) 3)verify that it works typing docker-compose --version --> it will return the same version of docker 4) pip unistall docker-compose Incapsulate docker images in a docker file using docker compose first information should be the base image used to spawn the containers an image consist of a rich prebuild application/software stack ready for the launch (i.e. ubuntu or python) docker compose add a new layer on the top of the base image used for each modification that we define in the services defined in the union file structure that we can modify after created an image need to be an image defined on the docker repository that docker engine can pull and update or modify FROM ubuntu --> we want contruct a new layer on top on ubuntu stack (could be one or multiple images) ADD ./code --> add the current directory into the path /code of the image WORKDIR /code --> set the working directory ENV PYTHONBUFFERED 1 --> enviroment variable definition RUN pip install -r requirement.txt --> perform an installation CMD python app.py --> command prompt to execute the application Definition of services in docker compose docker services, dependecies are defined in a config file named docker-compose.yml separate the dockerimage from the binding, port and volumes are very popular today and it is a good strategy docker compose use yml to define the basic docker services required for an application to load and execute containers each service defined in the compose file must specify exactly one image or build tags --> docker compose spawn multicontainer enviroment so each image must have the corresponding service entry only few keys are mandatory option defined in the dockerfile are automatically retrieved by the compose file docker ports--> [container_port]:[local host port] volumes --> data folder that can be reused -links--> links: -redis --> if we want link a container to the each other build from the docker file in the current directory in which the upser is operatig mount the current directory on the host to \"/code\" inside the container allows to modify the code without having to rebuild the image docker file --> which images, which folder to create yml file --> bind services with the component defined in the Dockerfile //DOCKERFILE EXAMPLE FOR DJANGO FROM python:2.7 ENV PYTHONBUFFERED 1 RUM mkdir /code WORKDIR /code ADD required.txt /code/ RUN pip install -r requirement.txt ADD . /code/ //DOCKERCOMPOSE YML FILE FOR DJANGO db: image: postgres web: build: . command: python manage.py runserver 0.0.0:8000 volumes: - .:/code ports: -\"8000.8000\" links: - db //REQUIREMENTS.TXT Django psycopg2 //driver that python use with postgres //put all together docker-compose run web django-admin.py startproject composeexample .","title":"dockerCompose"},{"location":"Development docs/Docker/Docker compose/dockerCompose/#install-docker-compose-on-ubuntu","text":"-not support windows os - docker installation and engine on required - grant permission for write/read ops --> best be root (cmd: su root) --> otherwise I need to chmod +x on the docker compose folder 1) grab the docker compose from github 2) using the pip utility python based utility --> pip install -U docker-compose (at first time is necessary update the system - do it automatically) 3)verify that it works typing docker-compose --version --> it will return the same version of docker 4) pip unistall docker-compose","title":"install docker compose on ubuntu"},{"location":"Development docs/Docker/Docker compose/dockerCompose/#incapsulate-docker-images-in-a-docker-file-using-docker-compose","text":"first information should be the base image used to spawn the containers an image consist of a rich prebuild application/software stack ready for the launch (i.e. ubuntu or python) docker compose add a new layer on the top of the base image used for each modification that we define in the services defined in the union file structure that we can modify after created an image need to be an image defined on the docker repository that docker engine can pull and update or modify FROM ubuntu --> we want contruct a new layer on top on ubuntu stack (could be one or multiple images) ADD ./code --> add the current directory into the path /code of the image WORKDIR /code --> set the working directory ENV PYTHONBUFFERED 1 --> enviroment variable definition RUN pip install -r requirement.txt --> perform an installation CMD python app.py --> command prompt to execute the application","title":"Incapsulate docker images in a docker file using docker compose"},{"location":"Development docs/Docker/Docker compose/dockerCompose/#definition-of-services-in-docker-compose","text":"docker services, dependecies are defined in a config file named docker-compose.yml separate the dockerimage from the binding, port and volumes are very popular today and it is a good strategy docker compose use yml to define the basic docker services required for an application to load and execute containers each service defined in the compose file must specify exactly one image or build tags --> docker compose spawn multicontainer enviroment so each image must have the corresponding service entry only few keys are mandatory option defined in the dockerfile are automatically retrieved by the compose file docker ports--> [container_port]:[local host port] volumes --> data folder that can be reused -links--> links: -redis --> if we want link a container to the each other build from the docker file in the current directory in which the upser is operatig mount the current directory on the host to \"/code\" inside the container allows to modify the code without having to rebuild the image docker file --> which images, which folder to create yml file --> bind services with the component defined in the Dockerfile //DOCKERFILE EXAMPLE FOR DJANGO FROM python:2.7 ENV PYTHONBUFFERED 1 RUM mkdir /code WORKDIR /code ADD required.txt /code/ RUN pip install -r requirement.txt ADD . /code/ //DOCKERCOMPOSE YML FILE FOR DJANGO db: image: postgres web: build: . command: python manage.py runserver 0.0.0:8000 volumes: - .:/code ports: -\"8000.8000\" links: - db //REQUIREMENTS.TXT Django psycopg2 //driver that python use with postgres //put all together docker-compose run web django-admin.py startproject composeexample .","title":"Definition of services in docker compose"},{"location":"Development docs/Docker/Labeling/label/","text":"Docker metadata treated as string store information away from the container runtime can be referenced by docker during the rt keyvalue useful for add description or notes store host port or other configuration options --> as labels --> label ark = \"heee\" key unique in the container context a-z-0-9-. lowercase use an unique namespace com.skill.blabla.someLabel com.dcoker or io.docker are namespace reserved --> namespace specific for the application structure data as label in Docker docker file is a file with instruction of how to build an image from the scratch new command for tune the image key value data as string opposite to json FROM ubuntu MANTAINER example LABEL Vendor=SkillSoft LABEL Version=1.0 LABEL com.example.image=\"{\\description\\:\\\"A basic intro\"} LABEL Year=2015 LABEL Color=blue //we can define string or integer vaie RUN apt-get update label created internally as json script -too many label is inefficient maintence of the container docker build - < Dockerfile I can inspect the labels with docker inspect --> into the json there is the label structure query labels docker images --> list of images builded --> i can search an image with a specific label docker images --filter \"label=Color=blue\"","title":"Label"},{"location":"Development docs/Docker/Labeling/label/#docker-metadata-treated-as-string","text":"store information away from the container runtime can be referenced by docker during the rt keyvalue useful for add description or notes store host port or other configuration options --> as labels --> label ark = \"heee\" key unique in the container context a-z-0-9-. lowercase use an unique namespace com.skill.blabla.someLabel com.dcoker or io.docker are namespace reserved --> namespace specific for the application","title":"Docker metadata treated as string"},{"location":"Development docs/Docker/Labeling/label/#structure-data-as-label-in-docker","text":"docker file is a file with instruction of how to build an image from the scratch new command for tune the image key value data as string opposite to json FROM ubuntu MANTAINER example LABEL Vendor=SkillSoft LABEL Version=1.0 LABEL com.example.image=\"{\\description\\:\\\"A basic intro\"} LABEL Year=2015 LABEL Color=blue //we can define string or integer vaie RUN apt-get update label created internally as json script -too many label is inefficient maintence of the container docker build - < Dockerfile I can inspect the labels with docker inspect --> into the json there is the label structure","title":"structure data as label in Docker"},{"location":"Development docs/Docker/Labeling/label/#query-labels","text":"docker images --> list of images builded --> i can search an image with a specific label docker images --filter \"label=Color=blue\"","title":"query labels"},{"location":"Development docs/Eclipse Che/randomNotes/","text":"presenter manage all the interaction of the user with the view and define how the view should be rendered the business logic specifically for the view is defined in its concrete implementation The presenter is very important in order to start all the process in order to see the dialog from a presenter you can bind the action delegate with the properly methods defined in the view implementation Inside the view implementation is necessary define the interface UI binder related to the specific xml layout for the dialog inside the view impl the action when user click on \"ok\" or \"close\" are managed by the ActionDelegate class with its methods onOkClicked and onCancelClicked Important hierarchy everything starts with the extension setting up its name defining the label of the group in the main menu of che it allow the ActionManager to register the actions that we need in our development // action are associated to a particular behavior defined by view implementation second important thing is the Action implementation class it extends BaseAction in its constructor we can set the title of the action in the group on the main menu toolbar there is an actionPerformed method that call the method defined by the presenter update method and setBrightsideExtension are to discuss later third part is the presenter in its constructor we put some contents: the interface View, AppContext and so on the presenter implements the ActionDelegate inner interface defined in the View interface, so we need to implement the methods onOkClicked and onCancelClicked (we usually use specific methods provided by the view interface) from the presenter is possible manage some commands to execute something. fourth part is relative to the View interface it extends its inner interface i.e. public interface HelloWorldView extends View there are the signs about specific methods (i.e. showDialog() and pippo()) The inner interface ActionDelegate contains two methods activated when user perform some operations on the dialog window (ok or close dialog) last part is the view implementation and its UI binder for a dialog the class extends Window and implement its interface parent If you define an object in the xml layout here you need to retreive it with the correct annotation --> @UiField TextBox memberField; In its constructor we define first of all the name of the dialog there is also a method AddFooterButton that is used to attach a button on the footer of the dialog. It return a Button object. In its click handler we call a method of the delegate class (ActionDelegate - the inner interface of the View class) THE IMPORTANT THING IS TO ATTACH THE XML TO THE VIEW IMPL USING THE METHOD createAndBindUi() that returns a Widget component that is settled in the Window instance (remember that the view impl extends Window class!) Also is necessary implement the view methods (for the showDialog method we can simply call the window method show()) Also we need an Interface that exteds UiBinder map that it requires the Widget object and the view impl. i.e. interface HelloWorldViewImplUiBinder extends UiBinder {} last part is relative to the xml layout there is a main root tag and inside of it we define: styling --> using DockLayoutPanel FlowPanel Specific objects (label, textbox etc)","title":"randomNotes"},{"location":"Development docs/Eclipse Che/Tree Management/SimpleTree/","text":"Manage tree in Eclipse Che The tree structure is very a popular choice every time that is necessary represent something that have a specific hierarchy between its elements. A project structure, a JSON representation of a REST call can be managed using the tree structure in a multi-level hierarchy. Eclipse Che provides out of the box a complex structure called SimpleTree that can be used to create plugins that represent some data into a tree visualization (as Che already defined with the explorer widget). Just to understand how this class provides the hierarchy will be defined a simple case relative to a tree with a root and three depth length, as reported below. The simple node structure Basically each element of our tree is a custom object defined with a set of attributes. In facts at this object (that extends the Node interface) is possible attach a custom icon, a text, a html selector and so on. `Java public class SimpleNode extends AbstractTreeNode implements HasNewPresentation { private NewNodePresentation.Builder nodePresentationBuilder; private NewNodePresentation newNodePresentation; private final String name; private boolean leaf; ... The abstract class AbstractTreeNode guarantees that the new node will be managed properly as node of a tree by the framework. But is necessary provide the override implementation of these methods: ```Java``` @Override public String getName() { ... } @Override public boolean isLeaf() { ... } @Override protected Promise<List<Node>> getChildrenImpl() { ... } In particular the last method is very interesting: it is called by the framework so the developer don't need to call it and it ask for the right implementation of the node (if it is a root with at least one parent there will a specific svg icon on the left). On the other hand this custom class also implements the interface HasNewPresentation that provides methods to define the graphic behavior of the node. Is necessary provide the implementations of the method getPresentation() Java @Override public NewNodePresentation getPresentation() { ... } ## More in deep discussion about SimpleNode class In order to create a node correctly is necessary as first thing define a name, and this could be archived with a parametric constructor or whit a setName method. The attribute name that are defined will be passed into the process that generate graphically the node. It will be discussed later. Another very important method is isLeaf: it accepts a boolean value, it should be defined as true if the node that will be created don't have any children (and in this case it will not have the specific svg icon). ```Java``` @Override public boolean isLeaf() { return leaf; } But the most important concept is understand that a node could be a root with (or without a child) or could be a child for a specific parent. In these cases the graphical representation will change. The abstract node class provide both a list of nodes and a parent node with the relative getter/setter that can be used to attach the new node created into the tree hierarchy. However in order to investigate about the list of children that could be defined for a selected parent is necessary implement the getChildrenImpl() as defined below: Java @SuppressWarnings(\"deprecation\") @Override protected Promise > getChildrenImpl() { return Promises.resolve(children); } For the presentation, basically is necessary implements the method getPresentation() using the inner class Builder provided by the class NewNodePresentation: ```Java``` @Override public NewNodePresentation getPresentation() { nodePresentationBuilder = new NewNodePresentation.Builder(); this.setNewNodePresentation(nodePresentationBuilder.withNodeText(this.getName()).build()); return this.getNewNodePresentation; } The full code of SimpleNode class is defined here [GIST] Load the nodes in the tree Now that the node element is defined, is only necessary initialize the tree structure and add the nodes inside of it. The tree object can be defined both into a dialog or into a panel, it doesn't matter (see the full code for more references). First of all is necessary create the nodes, passing their names in the class constructor. After that is necessary define the relations between the nodes and decide who are the leafs, as reported below: Java SimpleNode node0 = new SimpleNode(\"node-root\"); SimpleNode node1 = new SimpleNode(\"node-1\"); SimpleNode node2 = new SimpleNode(\"node-2\"); node0.setLeaf(false); node1.setLeaf(false); node2.setLeaf(true); node1.setParent(node0); node2.setParent(node1); List nodeRootChildrenList = new ArrayList (); nodeRootChildrenList.add(node1); node0.setChildren(nodeRootChildrenList); List nodeOneChildrenList = new ArrayList (); nodeOneChildrenList.add(node2); node1.setChildren(nodeOneChildrenList); Now it's possible create the tree object, that requires an instance of NodeStorage and NodeLoader that are define out of the box: ```Java``` NodeStorage nodeStorage = new NodeStorage(); NodeLoader nodeLoader = new NodeLoader(); treeHierarchy = new Tree(nodeStorage, nodeLoader); nodeStorage.add(node0); The add() method is necessary in order to create the root of the tree: for the other children of the root is not necessary call the add method because they are already defined in the children list in the SimpleNode instance and it will be retrieved during the rendering operation performed by Che. The full code about this implementation is here [GIST]","title":"Manage tree in Eclipse Che"},{"location":"Development docs/Eclipse Che/Tree Management/SimpleTree/#manage-tree-in-eclipse-che","text":"The tree structure is very a popular choice every time that is necessary represent something that have a specific hierarchy between its elements. A project structure, a JSON representation of a REST call can be managed using the tree structure in a multi-level hierarchy. Eclipse Che provides out of the box a complex structure called SimpleTree that can be used to create plugins that represent some data into a tree visualization (as Che already defined with the explorer widget). Just to understand how this class provides the hierarchy will be defined a simple case relative to a tree with a root and three depth length, as reported below.","title":"Manage tree in Eclipse Che"},{"location":"Development docs/Eclipse Che/Tree Management/SimpleTree/#the-simple-node-structure","text":"Basically each element of our tree is a custom object defined with a set of attributes. In facts at this object (that extends the Node interface) is possible attach a custom icon, a text, a html selector and so on. `Java public class SimpleNode extends AbstractTreeNode implements HasNewPresentation { private NewNodePresentation.Builder nodePresentationBuilder; private NewNodePresentation newNodePresentation; private final String name; private boolean leaf; ... The abstract class AbstractTreeNode guarantees that the new node will be managed properly as node of a tree by the framework. But is necessary provide the override implementation of these methods: ```Java``` @Override public String getName() { ... } @Override public boolean isLeaf() { ... } @Override protected Promise<List<Node>> getChildrenImpl() { ... } In particular the last method is very interesting: it is called by the framework so the developer don't need to call it and it ask for the right implementation of the node (if it is a root with at least one parent there will a specific svg icon on the left). On the other hand this custom class also implements the interface HasNewPresentation that provides methods to define the graphic behavior of the node. Is necessary provide the implementations of the method getPresentation() Java @Override public NewNodePresentation getPresentation() { ... } ## More in deep discussion about SimpleNode class In order to create a node correctly is necessary as first thing define a name, and this could be archived with a parametric constructor or whit a setName method. The attribute name that are defined will be passed into the process that generate graphically the node. It will be discussed later. Another very important method is isLeaf: it accepts a boolean value, it should be defined as true if the node that will be created don't have any children (and in this case it will not have the specific svg icon). ```Java``` @Override public boolean isLeaf() { return leaf; } But the most important concept is understand that a node could be a root with (or without a child) or could be a child for a specific parent. In these cases the graphical representation will change. The abstract node class provide both a list of nodes and a parent node with the relative getter/setter that can be used to attach the new node created into the tree hierarchy. However in order to investigate about the list of children that could be defined for a selected parent is necessary implement the getChildrenImpl() as defined below: Java @SuppressWarnings(\"deprecation\") @Override protected Promise > getChildrenImpl() { return Promises.resolve(children); } For the presentation, basically is necessary implements the method getPresentation() using the inner class Builder provided by the class NewNodePresentation: ```Java``` @Override public NewNodePresentation getPresentation() { nodePresentationBuilder = new NewNodePresentation.Builder(); this.setNewNodePresentation(nodePresentationBuilder.withNodeText(this.getName()).build()); return this.getNewNodePresentation; } The full code of SimpleNode class is defined here [GIST]","title":"The simple node structure"},{"location":"Development docs/Eclipse Che/Tree Management/SimpleTree/#load-the-nodes-in-the-tree","text":"Now that the node element is defined, is only necessary initialize the tree structure and add the nodes inside of it. The tree object can be defined both into a dialog or into a panel, it doesn't matter (see the full code for more references). First of all is necessary create the nodes, passing their names in the class constructor. After that is necessary define the relations between the nodes and decide who are the leafs, as reported below: Java SimpleNode node0 = new SimpleNode(\"node-root\"); SimpleNode node1 = new SimpleNode(\"node-1\"); SimpleNode node2 = new SimpleNode(\"node-2\"); node0.setLeaf(false); node1.setLeaf(false); node2.setLeaf(true); node1.setParent(node0); node2.setParent(node1); List nodeRootChildrenList = new ArrayList (); nodeRootChildrenList.add(node1); node0.setChildren(nodeRootChildrenList); List nodeOneChildrenList = new ArrayList (); nodeOneChildrenList.add(node2); node1.setChildren(nodeOneChildrenList); Now it's possible create the tree object, that requires an instance of NodeStorage and NodeLoader that are define out of the box: ```Java``` NodeStorage nodeStorage = new NodeStorage(); NodeLoader nodeLoader = new NodeLoader(); treeHierarchy = new Tree(nodeStorage, nodeLoader); nodeStorage.add(node0); The add() method is necessary in order to create the root of the tree: for the other children of the root is not necessary call the add method because they are already defined in the children list in the SimpleNode instance and it will be retrieved during the rendering operation performed by Che. The full code about this implementation is here [GIST]","title":"Load the nodes in the tree"},{"location":"Development docs/Java/JSON-RPC/json-rpc/","text":"JSON RPC standard it is a standard communication protocol between client and server. The communication can be bidirectional, it means that both client and server can send/receive messages from the opposite part the request is sent to a server that implements the protocol JSON-RPC is used to implement the LSP (Language Server Protocol) compilers can expose their services implementing interfaces provided by the Language Server tools can consume new services by connecting to the server. The communication is defined by JSON messages sharing that have a fixed structure: { \"jsonrpc\": \"2.0\", \"id\":0, \"method\":\"sayHelloWorld\", \"params\":[\"UserFirstName UserLastName\"] } method is the string with the name of the method to be invoked. params is an Object or Array of values to be passed as parameters to the defined method. id is a value of any type used to match the response with the request that it is replying to. And this could be a typical response from the server: content-type: application/json-rpc content-length: 60 date: Mon, 25 Jun 2018 08:20:15 GMT { \"jsonrpc\": \"2.0\", \"id\": 0, \"result\": \"Hello world, UserFirstName UserLastName\" } Underhood implementation RPC implementation in Java Jsonrpc4j is the most easy/important implementation of the JSON-RPC procedures for Java language. A good tutorial on how simply turn on the lights is available here Deploy on docker container Perform a build to create the jar file bash mvn package Create a Docker file with instruction on how spawn the image with the jar inside of it: FROM java:8 WORKDIR / ADD bootrpc-0.1.0.jar bootrpc-0.1.0.jar EXPOSE 8080 CMD java -jar bootrpc-0.1.0.jar Spawn the image with the command: docker build . -f Dockerfile -t bootrpc Run the container: docker run bootrpc Verify that the container is up: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9dead33f5efa bootrpc \"/bin/sh -c 'java -j\u2026\" 2 hours ago Up 2 hours 8080/tcp jovial_euler Perform a Curl to verify that everything works as expected: curl -H \"Content-Type:application/json\" -d '{\"id\":0, \"method\":\"sayHelloWorld\", \"params\":[\"Andera Doe\"]}' http://172.17.0.2:8080/rpc/myservice If everything works fine we can receive a response with '200 ok' http response code with a JSON in the response body: {\"jsonrpc\": \"2.0\", \"result\": \"Hello visitor!\", \"id\": 3}","title":"JSON RPC standard"},{"location":"Development docs/Java/JSON-RPC/json-rpc/#json-rpc-standard","text":"it is a standard communication protocol between client and server. The communication can be bidirectional, it means that both client and server can send/receive messages from the opposite part the request is sent to a server that implements the protocol JSON-RPC is used to implement the LSP (Language Server Protocol) compilers can expose their services implementing interfaces provided by the Language Server tools can consume new services by connecting to the server. The communication is defined by JSON messages sharing that have a fixed structure: { \"jsonrpc\": \"2.0\", \"id\":0, \"method\":\"sayHelloWorld\", \"params\":[\"UserFirstName UserLastName\"] } method is the string with the name of the method to be invoked. params is an Object or Array of values to be passed as parameters to the defined method. id is a value of any type used to match the response with the request that it is replying to. And this could be a typical response from the server: content-type: application/json-rpc content-length: 60 date: Mon, 25 Jun 2018 08:20:15 GMT { \"jsonrpc\": \"2.0\", \"id\": 0, \"result\": \"Hello world, UserFirstName UserLastName\" }","title":"JSON RPC standard"},{"location":"Development docs/Java/JSON-RPC/json-rpc/#underhood-implementation","text":"","title":"Underhood implementation"},{"location":"Development docs/Java/JSON-RPC/json-rpc/#rpc-implementation-in-java","text":"Jsonrpc4j is the most easy/important implementation of the JSON-RPC procedures for Java language. A good tutorial on how simply turn on the lights is available here","title":"RPC implementation in Java"},{"location":"Development docs/Java/JSON-RPC/json-rpc/#deploy-on-docker-container","text":"Perform a build to create the jar file bash mvn package Create a Docker file with instruction on how spawn the image with the jar inside of it: FROM java:8 WORKDIR / ADD bootrpc-0.1.0.jar bootrpc-0.1.0.jar EXPOSE 8080 CMD java -jar bootrpc-0.1.0.jar Spawn the image with the command: docker build . -f Dockerfile -t bootrpc Run the container: docker run bootrpc Verify that the container is up: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9dead33f5efa bootrpc \"/bin/sh -c 'java -j\u2026\" 2 hours ago Up 2 hours 8080/tcp jovial_euler Perform a Curl to verify that everything works as expected: curl -H \"Content-Type:application/json\" -d '{\"id\":0, \"method\":\"sayHelloWorld\", \"params\":[\"Andera Doe\"]}' http://172.17.0.2:8080/rpc/myservice If everything works fine we can receive a response with '200 ok' http response code with a JSON in the response body: {\"jsonrpc\": \"2.0\", \"result\": \"Hello visitor!\", \"id\": 3}","title":"Deploy on docker container"},{"location":"Development docs/Java/Java-8 functionalities/ExecutorService/","text":"ExecutorService ExecutorService is a framework provided by JDK that simplify the execution of async tasks - It defines a thread pool - The API assign tasks to each thread ExecutorService is an interface, so is necessary use an its implementation, using the factory method provided by the framework, i.e.: ExecutorService executor = Executors.newFixedThreadPool(...); ExecutorService executor = Executors.newCachedThreadPool(); The cached thread pool approach could be very useful for performance matters, because it's possible reuse previous threads. Its factory method create a new object ThreadPoolExecutor : public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue<Runnable>()); } The ThreadPoolExecutor object is a class that extends abstract class AbstractExecutorService that implements interface ExecutorService It requires some parameters (that are defined by the factory method newCachedThreadPool() out of the box): int corePoolSize int maximumPoolSize long keepAliveTime TimeUnit unit BlockingQueue< Runnable > workQueue From javadoc: /*** * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless {@code allowCoreThreadTimeOut} is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the {@code keepAliveTime} argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the {@code Runnable} * tasks submitted by the {@code execute} method. * @throws IllegalArgumentException if one of the following holds:<br> * {@code corePoolSize < 0}<br> * {@code keepAliveTime < 0}<br> * {@code maximumPoolSize <= 0}<br> * {@code maximumPoolSize < corePoolSize} * @throws NullPointerException if {@code workQueue} is null ***/ ``` An ExecutorService can execute Runnable and Callable tasks. >Both Runnable and Callable are defined for perform async tasks, but in a Runnable operation, the task result is not returned back after its finish. Instead the Callable object is defined for tasks that after its finish return back a result that is necessary handle (i.e. status code for a HTTP operation). ```java Runnable myRunnableTask = () -> { try { TimeUnit.MILLISECONDS.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); } }; //lambda version Callable<String> myCallableTask = () -> { TimeUnit.MILLISECONDS.sleep(300); return \"Callable task execution\"; }; /* //innerclass callable Callable<String> myCallableTask = new Callable<String>() { @Override public String call() throws Exception { TimeUnit.MILLISECONDS.sleep(300); return \"Callable task execution\"; } }; */ List<Callable<String>> callableTasksList = new ArrayList<>(); callableTasksList.add(myCallableTask); callableTasks.add(myCallableTask); callableTasks.add(myCallableTask); //execute a runnable task executorService.execute(myRunnableTask); //execute single or multi callable tasks and get the result Future<String> future = executorService.submit(callableTask); List<Future<String>> futures = executorService.invokeAll(callableTasks); execute() is void and it not provides anything to get the result of the task's execution or check the status (if the task is complete or not) submit(), invokeAll() submit the task execution (that could be both Runnable or Callable tasks) to the ExecutorService and return the Future or list of Future object that handle the result of the task's execution. Managing the Future object, in order to grab the result is possible invoke the method future.get() and is also possible define a time limit for retreive the result: String result = future.get(200, TimeUnit.MILLISECONDS); In order to check the completion status there are several methods as isDone() or isCancelled()","title":"ExecutorService"},{"location":"Development docs/Java/Java-8 functionalities/ExecutorService/#executorservice","text":"ExecutorService is a framework provided by JDK that simplify the execution of async tasks - It defines a thread pool - The API assign tasks to each thread ExecutorService is an interface, so is necessary use an its implementation, using the factory method provided by the framework, i.e.: ExecutorService executor = Executors.newFixedThreadPool(...); ExecutorService executor = Executors.newCachedThreadPool(); The cached thread pool approach could be very useful for performance matters, because it's possible reuse previous threads. Its factory method create a new object ThreadPoolExecutor : public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue<Runnable>()); } The ThreadPoolExecutor object is a class that extends abstract class AbstractExecutorService that implements interface ExecutorService It requires some parameters (that are defined by the factory method newCachedThreadPool() out of the box): int corePoolSize int maximumPoolSize long keepAliveTime TimeUnit unit BlockingQueue< Runnable > workQueue From javadoc: /*** * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless {@code allowCoreThreadTimeOut} is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the {@code keepAliveTime} argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the {@code Runnable} * tasks submitted by the {@code execute} method. * @throws IllegalArgumentException if one of the following holds:<br> * {@code corePoolSize < 0}<br> * {@code keepAliveTime < 0}<br> * {@code maximumPoolSize <= 0}<br> * {@code maximumPoolSize < corePoolSize} * @throws NullPointerException if {@code workQueue} is null ***/ ``` An ExecutorService can execute Runnable and Callable tasks. >Both Runnable and Callable are defined for perform async tasks, but in a Runnable operation, the task result is not returned back after its finish. Instead the Callable object is defined for tasks that after its finish return back a result that is necessary handle (i.e. status code for a HTTP operation). ```java Runnable myRunnableTask = () -> { try { TimeUnit.MILLISECONDS.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); } }; //lambda version Callable<String> myCallableTask = () -> { TimeUnit.MILLISECONDS.sleep(300); return \"Callable task execution\"; }; /* //innerclass callable Callable<String> myCallableTask = new Callable<String>() { @Override public String call() throws Exception { TimeUnit.MILLISECONDS.sleep(300); return \"Callable task execution\"; } }; */ List<Callable<String>> callableTasksList = new ArrayList<>(); callableTasksList.add(myCallableTask); callableTasks.add(myCallableTask); callableTasks.add(myCallableTask); //execute a runnable task executorService.execute(myRunnableTask); //execute single or multi callable tasks and get the result Future<String> future = executorService.submit(callableTask); List<Future<String>> futures = executorService.invokeAll(callableTasks); execute() is void and it not provides anything to get the result of the task's execution or check the status (if the task is complete or not) submit(), invokeAll() submit the task execution (that could be both Runnable or Callable tasks) to the ExecutorService and return the Future or list of Future object that handle the result of the task's execution. Managing the Future object, in order to grab the result is possible invoke the method future.get() and is also possible define a time limit for retreive the result: String result = future.get(200, TimeUnit.MILLISECONDS); In order to check the completion status there are several methods as isDone() or isCancelled()","title":"ExecutorService"},{"location":"Development docs/Java/Java-8 functionalities/ForkJoin/","text":"Fork/Join framework It's a new framework for async multithread task operation introduced by Java 1.7 using the processor cores to split the operations and run it in a very fast mode. It is basically a different implementation of ExecutorService interface. Main concepts The operation is divided into two phases: fork phase where the problem is divided in more simply tasks until the task is quite simple to be executed by an async operation join phase where all the results executed by the async tasks are retrieved, all merged into the final result Fork/Join framework use a pool ForkJoinPool that manage thread defined as ForkJoinWorkerThread objects. ForkJoin pool It's a pool that manage WorkerThread instances. The pool doesn't create a new thread for each worker, but each thread have a queue of tasks that are executed by the worker from the tail of the queue. The interesting thing is that if a worker have completed its tasks, it can execute tasks defined on bottom of other busy worker's queue. In this way the probability that two threads will be in concurrency for a task is reduced. This process is called Work Stealing Algorithm .","title":"Fork/Join framework"},{"location":"Development docs/Java/Java-8 functionalities/ForkJoin/#forkjoin-framework","text":"It's a new framework for async multithread task operation introduced by Java 1.7 using the processor cores to split the operations and run it in a very fast mode. It is basically a different implementation of ExecutorService interface.","title":"Fork/Join framework"},{"location":"Development docs/Java/Java-8 functionalities/ForkJoin/#main-concepts","text":"The operation is divided into two phases: fork phase where the problem is divided in more simply tasks until the task is quite simple to be executed by an async operation join phase where all the results executed by the async tasks are retrieved, all merged into the final result Fork/Join framework use a pool ForkJoinPool that manage thread defined as ForkJoinWorkerThread objects.","title":"Main concepts"},{"location":"Development docs/Java/Java-8 functionalities/ForkJoin/#forkjoin-pool","text":"It's a pool that manage WorkerThread instances. The pool doesn't create a new thread for each worker, but each thread have a queue of tasks that are executed by the worker from the tail of the queue. The interesting thing is that if a worker have completed its tasks, it can execute tasks defined on bottom of other busy worker's queue. In this way the probability that two threads will be in concurrency for a task is reduced. This process is called Work Stealing Algorithm .","title":"ForkJoin pool"},{"location":"Development docs/Java/Java-8 functionalities/Future/","text":"Multithread with CompletableFuture With Java 8 a new feature is available to help developers to code in asynchronus way. From the package java.util.concurrent are now available the class CompletableFuture<T> that implements both interfaces Future<T> and CompletionStage<T> A Future represent the pending result of an asynchronus operation and CompletationStage is a Promise. Promise that the computation will be done in the future Simple example public class Foo{ //... public string doSomething(){ return \"hey!\"} public void startFoo(){ CompletableFuture<String> result = CompletableFuture.supplyAsync(this::doSomething); } }","title":"Multithread with CompletableFuture"},{"location":"Development docs/Java/Java-8 functionalities/Future/#multithread-with-completablefuture","text":"With Java 8 a new feature is available to help developers to code in asynchronus way. From the package java.util.concurrent are now available the class CompletableFuture<T> that implements both interfaces Future<T> and CompletionStage<T> A Future represent the pending result of an asynchronus operation and CompletationStage is a Promise. Promise that the computation will be done in the future","title":"Multithread with CompletableFuture"},{"location":"Development docs/Java/Java-8 functionalities/Future/#simple-example","text":"public class Foo{ //... public string doSomething(){ return \"hey!\"} public void startFoo(){ CompletableFuture<String> result = CompletableFuture.supplyAsync(this::doSomething); } }","title":"Simple example"},{"location":"Development docs/Language Server/ClientServerHandshake/","text":"Manage client-server communication with LSP4J When two actors (clients and server) compliant to the LSP requirements need to communicate together they need a launcher a Launcher is basically the entry point for applications that will be compliant with the LSP standard. LSP4J provides different kind of Launcher that implement the communication through the stdio channels. But is also possible use the stdio channels provided by a socket. // wait for clients to connect on port 1044 try(ServerSocket serverSocket = new ServerSocket(port)) { Socket socket = serverSocket.accept(); Launcher<LanguageClient> launcher = LSPLauncher.createServerLauncher( myServer, socket.getInputStream(), socket.getOutputStream()); // add clients to the server Runnable addClient = myServer.setRemoteProxy(launcher.getRemoteProxy()); launcher.startListening(); CompletableFuture.runAsync(addClient); } In order to be able to define a multithread LS is necessary wrap the previous code snippet into a callable object. Each thread (that could be defined using different implementations of ExecutorService interface) will execute the callable task. ExecutorService threadPool = Executors.newCachedThreadPool(); Integer port = 1044; Callable<Void> callableTask = new Callable<Void>() { @Override public Void call() throws Exception { while (true) { try(ServerSocket serverSocket = new ServerSocket(port)) { Socket socket = serverSocket.accept(); Launcher<LanguageClient> launcher = LSPLauncher.createServerLauncher( myServer, socket.getInputStream(), socket.getOutputStream()); // add clients to the server Runnable addClient = myServer.setRemoteProxy(launcher.getRemoteProxy()); launcher.startListening(); CompletableFuture.runAsync(addClient); } } } }; threadPool.submit(callableTask); The server needs to understand who is the client that asked to communicate with it. For this reason inside the callable activity there is an async task that execute a Runnable in order to add the LanguageClient retrieved from the launcher into the server instance that will use this class to access to the methods that it needs to use to send notification messages and other things to the connected client. The Launcher The launcher object provides all the underwood configuration in order to connect the remote endpoint via input and output stream. In particular the LSP launcher need some parameters: - local service = the concrete server implementation - remote interface = the LanguageClient class - input stream - output stream Into the launcher is defined a builder that provides wires up all components necessary for the JSON-RPC communication. /* @param localServices - the objects that receive method calls from the remote services * @param remoteInterfaces - interfaces on which RPC methods are looked up * @param classLoader - a class loader that is able to resolve all given interfaces * @param in - input stream to listen for incoming messages * @param out - output stream to send outgoing messages * @param executorService - the executor service used to start threads * @param wrapper - a function for plugging in additional message consumers * @param configureGson - a function for Gson configuration */ static Launcher<Object> createIoLauncher(Collection<Object> localServices, Collection<Class<?>> remoteInterfaces, ClassLoader classLoader, InputStream in, OutputStream out, ExecutorService executorService, Function<MessageConsumer, MessageConsumer> wrapper, Consumer<GsonBuilder> configureGson) { return new Builder<Object>() .setLocalServices(localServices) .setRemoteInterfaces(remoteInterfaces) .setClassLoader(classLoader) .setInput(in) .setOutput(out) .setExecutorService(executorService) .wrapMessages(wrapper) .configureGson(configureGson) .create(); } Builder - create() The create() method is entitled to define all the internal structures: public Launcher<T> create() { if (input == null) throw new IllegalStateException(\"Input stream must be configured.\"); if (output == null) throw new IllegalStateException(\"Output stream must be configured.\"); if (localServices == null) throw new IllegalStateException(\"Local service must be configured.\"); if (remoteInterfaces == null) throw new IllegalStateException(\"Remote interface must be configured.\"); MessageJsonHandler jsonHandler = createJsonHandler(); RemoteEndpoint remoteEndpoint = createRemoteEndpoint(jsonHandler); T remoteProxy; if (localServices.size() == 1 && remoteInterfaces.size() == 1) { remoteProxy = ServiceEndpoints.toServiceObject(remoteEndpoint, remoteInterfaces.iterator().next()); } else { remoteProxy = (T) ServiceEndpoints.toServiceObject(remoteEndpoint, (Collection<Class<?>>) (Object) remoteInterfaces, classLoader); } StreamMessageProducer reader = new StreamMessageProducer(input, jsonHandler, remoteEndpoint); MessageConsumer messageConsumer = wrapMessageConsumer(remoteEndpoint); ExecutorService execService = executorService != null ? executorService : Executors.newCachedThreadPool(); return new Launcher<T> () { @Override public Future<Void> startListening() { return ConcurrentMessageProcessor.startProcessing(reader, messageConsumer, execService); } @Override public T getRemoteProxy() { return remoteProxy; } @Override public RemoteEndpoint getRemoteEndpoint() { return remoteEndpoint; } }; } Builder - createJsonHandler() /** * Create the JSON handler for messages between the local and remote services. */ protected MessageJsonHandler createJsonHandler() { Map<String, JsonRpcMethod> supportedMethods = getSupportedMethods(); if (configureGson != null) return new MessageJsonHandler(supportedMethods, configureGson); else return new MessageJsonHandler(supportedMethods); } Builder - getSupportedMethods() /** * Gather all JSON-RPC methods from the local and remote services. */ protected Map<String, JsonRpcMethod> getSupportedMethods() { Map<String, JsonRpcMethod> supportedMethods = new LinkedHashMap<>(); // Gather the supported methods of remote interfaces for (Class<?> interface_ : remoteInterfaces) { supportedMethods.putAll(ServiceEndpoints.getSupportedMethods(interface_)); } // Gather the supported methods of local services for (Object localService : localServices) { if (localService instanceof JsonRpcMethodProvider) { JsonRpcMethodProvider rpcMethodProvider = (JsonRpcMethodProvider) localService; supportedMethods.putAll(rpcMethodProvider.supportedMethods()); } else { supportedMethods.putAll(ServiceEndpoints.getSupportedMethods(localService.getClass())); } } return supportedMethods; } Builder - createRemoteEndpoint() /** * Create the remote endpoint that communicates with the local services. */ protected RemoteEndpoint createRemoteEndpoint(MessageJsonHandler jsonHandler) { MessageConsumer outgoingMessageStream = new StreamMessageConsumer(output, jsonHandler); outgoingMessageStream = wrapMessageConsumer(outgoingMessageStream); RemoteEndpoint remoteEndpoint = new RemoteEndpoint(outgoingMessageStream, ServiceEndpoints.toEndpoint(localServices)); jsonHandler.setMethodProvider(remoteEndpoint); return remoteEndpoint; } Extra - RemoteEndpoint class /** * An endpoint that can be used to send messages to a given {@link MessageConsumer} by calling * {@link #request(String, Object)} or {@link #notify(String, Object)}. When connected to a {@link MessageProducer}, * this class forwards received messages to the local {@link Endpoint} given in the constructor. */ public class RemoteEndpoint implements Endpoint, MessageConsumer, MessageIssueHandler, MethodProvider { private static final Logger LOG = Logger.getLogger(RemoteEndpoint.class.getName()); public static final Function<Throwable, ResponseError> DEFAULT_EXCEPTION_HANDLER = (throwable) -> { if (throwable instanceof ResponseErrorException) { return ((ResponseErrorException) throwable).getResponseError(); } else if ((throwable instanceof CompletionException || throwable instanceof InvocationTargetException) && throwable.getCause() instanceof ResponseErrorException) { return ((ResponseErrorException) throwable.getCause()).getResponseError(); } else { return fallbackResponseError(\"Internal error\", throwable); } }; private static ResponseError fallbackResponseError(String header, Throwable throwable) { LOG.log(Level.SEVERE, header + \": \" + throwable.getMessage(), throwable); ResponseError error = new ResponseError(); error.setMessage(header + \".\"); error.setCode(ResponseErrorCode.InternalError); ByteArrayOutputStream stackTrace = new ByteArrayOutputStream(); PrintWriter stackTraceWriter = new PrintWriter(stackTrace); throwable.printStackTrace(stackTraceWriter); stackTraceWriter.flush(); error.setData(stackTrace.toString()); return error; } private final MessageConsumer out; private final Endpoint localEndpoint; private final Function<Throwable, ResponseError> exceptionHandler; private final AtomicInteger nextRequestId = new AtomicInteger(); private final Map<String, PendingRequestInfo> sentRequestMap = new LinkedHashMap<>(); private final Map<String, CompletableFuture<?>> receivedRequestMap = new LinkedHashMap<>(); //... //constructor area /** * @param out - a consumer that transmits messages to the remote service * @param localEndpoint - the local service implementation * @param exceptionHandler - an exception handler that should never return null. */ public RemoteEndpoint(MessageConsumer out, Endpoint localEndpoint, Function<Throwable, ResponseError> exceptionHandler) { if (out == null) throw new NullPointerException(\"out\"); if (localEndpoint == null) throw new NullPointerException(\"localEndpoint\"); if (exceptionHandler == null) throw new NullPointerException(\"exceptionHandler\"); this.out = out; this.localEndpoint = localEndpoint; this.exceptionHandler = exceptionHandler; } /** * @param out - a consumer that transmits messages to the remote service * @param localEndpoint - the local service implementation */ public RemoteEndpoint(MessageConsumer out, Endpoint localEndpoint) { this(out, localEndpoint, DEFAULT_EXCEPTION_HANDLER); } // ... other methods ... } Builder operations workflow Create an instance of MessageJsonHandler (using internal method createJsonHandler() ): MessageJsonHandler works as a wrapper around Gson that includes configuration necessary for the RPC communication. Using its routine getSupportedMethods() the MessageJsonHandler will be populated with the list of all RPC supported methods both for client and server. With the defined MessageJsonHandler object, the builder invoke the createRemoteEndpoint() method. Here a new Endpoint is created with a defined MessageConsumer and Endpoint defined. Note ServiceEndpoints.toEndpoint(localServices) is a static method defined by the ServiceEndpoint class and it wraps a given object with service annotations behind an Endpoint interface. /** * Wraps a collection of objects with service annotations behind an {@link Endpoint} interface. * * @return the wrapped service endpoint */ public static Endpoint toEndpoint(Collection<Object> serviceObjects) { return new GenericEndpoint(serviceObjects); } Next is necessary define the remoteProxy object that will be used by the LS to set the LanguageClient class (that will be used to inspect methods available that the server will use to send messages to the server) In order to define the RemoteProxy a static method provided by ServiceEndpoints will be used. This routine wraps a given Endpoint in the given service interface. java /** * Wraps a given {@link Endpoint} in the given service interface. * * @return the wrapped service object */ @SuppressWarnings(\"unchecked\") public static <T> T toServiceObject(Endpoint endpoint, Class<T> interface_) { Class<?>[] interfArray = new Class[]{interface_, Endpoint.class}; EndpointProxy invocationHandler = new EndpointProxy(endpoint, interface_); return (T) Proxy.newProxyInstance(interface_.getClassLoader(), interfArray, invocationHandler); } 4. After that is necessary define the MessageConsumer that reads from an input stream and parses messages from JSON 5. Finally is necessary define a new ExecutorService or use one already defined and sent to the createLauncher() method 6. After all these steps, a new anonymous class for Launcher<T> is created and are also defined methods that the new class need to override from the Launcher interface: @SuppressWarnings(\"unchecked\") public Launcher<T> create() { //... return new Launcher<T> () { @Override public Future<Void> startListening() { return ConcurrentMessageProcessor.startProcessing(reader, messageConsumer, execService); } @Override public T getRemoteProxy() { return remoteProxy; } @Override public RemoteEndpoint getRemoteEndpoint() { return remoteEndpoint; } }; } startListening() & startProcessing() method The startListening() method is defined by each class that implements the Launcher interface, and also anonymous classes. This method invoke the static startProcessing() method defined by the class ConcurrentMessageProcessor , defined into the package org.eclipse.lsp4j.jsonrpc.json . The class ConcurrentMessageProcessor implements the Runnable interface and its static method startProcessing return a Future<Void> that will be resolved when the started thread is terminated: /** * Start a thread that listens for messages in the message producer and forwards them to the message consumer. * * @param messageProducer - produces messages, e.g. by reading from an input channel * @param messageConsumer - processes messages and potentially forwards them to other consumers * @param executorService - the thread is started using this service * @return a future that is resolved when the started thread is terminated, e.g. by closing a stream */ public static Future<Void> startProcessing(MessageProducer messageProducer, MessageConsumer messageConsumer, ExecutorService executorService) { ConcurrentMessageProcessor reader = new ConcurrentMessageProcessor(messageProducer, messageConsumer); final Future<?> result = executorService.submit(reader); return new Future<Void>() { @Override public Void get() throws InterruptedException, ExecutionException { return (Void) result.get(); } @Override public Void get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException { return (Void) result.get(timeout, unit); } @Override public boolean isDone() { return result.isDone(); } @Override public boolean cancel(boolean mayInterruptIfRunning) { if (mayInterruptIfRunning && messageProducer instanceof Closeable) { try { ((Closeable) messageProducer).close(); } catch (IOException e) { throw new RuntimeException(e); } } return result.cancel(mayInterruptIfRunning); } @Override public boolean isCancelled() { return result.isCancelled(); } };","title":"Manage client-server communication with LSP4J"},{"location":"Development docs/Language Server/ClientServerHandshake/#manage-client-server-communication-with-lsp4j","text":"When two actors (clients and server) compliant to the LSP requirements need to communicate together they need a launcher a Launcher is basically the entry point for applications that will be compliant with the LSP standard. LSP4J provides different kind of Launcher that implement the communication through the stdio channels. But is also possible use the stdio channels provided by a socket. // wait for clients to connect on port 1044 try(ServerSocket serverSocket = new ServerSocket(port)) { Socket socket = serverSocket.accept(); Launcher<LanguageClient> launcher = LSPLauncher.createServerLauncher( myServer, socket.getInputStream(), socket.getOutputStream()); // add clients to the server Runnable addClient = myServer.setRemoteProxy(launcher.getRemoteProxy()); launcher.startListening(); CompletableFuture.runAsync(addClient); } In order to be able to define a multithread LS is necessary wrap the previous code snippet into a callable object. Each thread (that could be defined using different implementations of ExecutorService interface) will execute the callable task. ExecutorService threadPool = Executors.newCachedThreadPool(); Integer port = 1044; Callable<Void> callableTask = new Callable<Void>() { @Override public Void call() throws Exception { while (true) { try(ServerSocket serverSocket = new ServerSocket(port)) { Socket socket = serverSocket.accept(); Launcher<LanguageClient> launcher = LSPLauncher.createServerLauncher( myServer, socket.getInputStream(), socket.getOutputStream()); // add clients to the server Runnable addClient = myServer.setRemoteProxy(launcher.getRemoteProxy()); launcher.startListening(); CompletableFuture.runAsync(addClient); } } } }; threadPool.submit(callableTask); The server needs to understand who is the client that asked to communicate with it. For this reason inside the callable activity there is an async task that execute a Runnable in order to add the LanguageClient retrieved from the launcher into the server instance that will use this class to access to the methods that it needs to use to send notification messages and other things to the connected client.","title":"Manage client-server communication with LSP4J"},{"location":"Development docs/Language Server/ClientServerHandshake/#the-launcher","text":"The launcher object provides all the underwood configuration in order to connect the remote endpoint via input and output stream. In particular the LSP launcher need some parameters: - local service = the concrete server implementation - remote interface = the LanguageClient class - input stream - output stream Into the launcher is defined a builder that provides wires up all components necessary for the JSON-RPC communication. /* @param localServices - the objects that receive method calls from the remote services * @param remoteInterfaces - interfaces on which RPC methods are looked up * @param classLoader - a class loader that is able to resolve all given interfaces * @param in - input stream to listen for incoming messages * @param out - output stream to send outgoing messages * @param executorService - the executor service used to start threads * @param wrapper - a function for plugging in additional message consumers * @param configureGson - a function for Gson configuration */ static Launcher<Object> createIoLauncher(Collection<Object> localServices, Collection<Class<?>> remoteInterfaces, ClassLoader classLoader, InputStream in, OutputStream out, ExecutorService executorService, Function<MessageConsumer, MessageConsumer> wrapper, Consumer<GsonBuilder> configureGson) { return new Builder<Object>() .setLocalServices(localServices) .setRemoteInterfaces(remoteInterfaces) .setClassLoader(classLoader) .setInput(in) .setOutput(out) .setExecutorService(executorService) .wrapMessages(wrapper) .configureGson(configureGson) .create(); }","title":"The Launcher"},{"location":"Development docs/Language Server/ClientServerHandshake/#builder-create","text":"The create() method is entitled to define all the internal structures: public Launcher<T> create() { if (input == null) throw new IllegalStateException(\"Input stream must be configured.\"); if (output == null) throw new IllegalStateException(\"Output stream must be configured.\"); if (localServices == null) throw new IllegalStateException(\"Local service must be configured.\"); if (remoteInterfaces == null) throw new IllegalStateException(\"Remote interface must be configured.\"); MessageJsonHandler jsonHandler = createJsonHandler(); RemoteEndpoint remoteEndpoint = createRemoteEndpoint(jsonHandler); T remoteProxy; if (localServices.size() == 1 && remoteInterfaces.size() == 1) { remoteProxy = ServiceEndpoints.toServiceObject(remoteEndpoint, remoteInterfaces.iterator().next()); } else { remoteProxy = (T) ServiceEndpoints.toServiceObject(remoteEndpoint, (Collection<Class<?>>) (Object) remoteInterfaces, classLoader); } StreamMessageProducer reader = new StreamMessageProducer(input, jsonHandler, remoteEndpoint); MessageConsumer messageConsumer = wrapMessageConsumer(remoteEndpoint); ExecutorService execService = executorService != null ? executorService : Executors.newCachedThreadPool(); return new Launcher<T> () { @Override public Future<Void> startListening() { return ConcurrentMessageProcessor.startProcessing(reader, messageConsumer, execService); } @Override public T getRemoteProxy() { return remoteProxy; } @Override public RemoteEndpoint getRemoteEndpoint() { return remoteEndpoint; } }; }","title":"Builder - create()"},{"location":"Development docs/Language Server/ClientServerHandshake/#builder-createjsonhandler","text":"/** * Create the JSON handler for messages between the local and remote services. */ protected MessageJsonHandler createJsonHandler() { Map<String, JsonRpcMethod> supportedMethods = getSupportedMethods(); if (configureGson != null) return new MessageJsonHandler(supportedMethods, configureGson); else return new MessageJsonHandler(supportedMethods); }","title":"Builder - createJsonHandler()"},{"location":"Development docs/Language Server/ClientServerHandshake/#builder-getsupportedmethods","text":"/** * Gather all JSON-RPC methods from the local and remote services. */ protected Map<String, JsonRpcMethod> getSupportedMethods() { Map<String, JsonRpcMethod> supportedMethods = new LinkedHashMap<>(); // Gather the supported methods of remote interfaces for (Class<?> interface_ : remoteInterfaces) { supportedMethods.putAll(ServiceEndpoints.getSupportedMethods(interface_)); } // Gather the supported methods of local services for (Object localService : localServices) { if (localService instanceof JsonRpcMethodProvider) { JsonRpcMethodProvider rpcMethodProvider = (JsonRpcMethodProvider) localService; supportedMethods.putAll(rpcMethodProvider.supportedMethods()); } else { supportedMethods.putAll(ServiceEndpoints.getSupportedMethods(localService.getClass())); } } return supportedMethods; }","title":"Builder - getSupportedMethods()"},{"location":"Development docs/Language Server/ClientServerHandshake/#builder-createremoteendpoint","text":"/** * Create the remote endpoint that communicates with the local services. */ protected RemoteEndpoint createRemoteEndpoint(MessageJsonHandler jsonHandler) { MessageConsumer outgoingMessageStream = new StreamMessageConsumer(output, jsonHandler); outgoingMessageStream = wrapMessageConsumer(outgoingMessageStream); RemoteEndpoint remoteEndpoint = new RemoteEndpoint(outgoingMessageStream, ServiceEndpoints.toEndpoint(localServices)); jsonHandler.setMethodProvider(remoteEndpoint); return remoteEndpoint; }","title":"Builder - createRemoteEndpoint()"},{"location":"Development docs/Language Server/ClientServerHandshake/#extra-remoteendpoint-class","text":"/** * An endpoint that can be used to send messages to a given {@link MessageConsumer} by calling * {@link #request(String, Object)} or {@link #notify(String, Object)}. When connected to a {@link MessageProducer}, * this class forwards received messages to the local {@link Endpoint} given in the constructor. */ public class RemoteEndpoint implements Endpoint, MessageConsumer, MessageIssueHandler, MethodProvider { private static final Logger LOG = Logger.getLogger(RemoteEndpoint.class.getName()); public static final Function<Throwable, ResponseError> DEFAULT_EXCEPTION_HANDLER = (throwable) -> { if (throwable instanceof ResponseErrorException) { return ((ResponseErrorException) throwable).getResponseError(); } else if ((throwable instanceof CompletionException || throwable instanceof InvocationTargetException) && throwable.getCause() instanceof ResponseErrorException) { return ((ResponseErrorException) throwable.getCause()).getResponseError(); } else { return fallbackResponseError(\"Internal error\", throwable); } }; private static ResponseError fallbackResponseError(String header, Throwable throwable) { LOG.log(Level.SEVERE, header + \": \" + throwable.getMessage(), throwable); ResponseError error = new ResponseError(); error.setMessage(header + \".\"); error.setCode(ResponseErrorCode.InternalError); ByteArrayOutputStream stackTrace = new ByteArrayOutputStream(); PrintWriter stackTraceWriter = new PrintWriter(stackTrace); throwable.printStackTrace(stackTraceWriter); stackTraceWriter.flush(); error.setData(stackTrace.toString()); return error; } private final MessageConsumer out; private final Endpoint localEndpoint; private final Function<Throwable, ResponseError> exceptionHandler; private final AtomicInteger nextRequestId = new AtomicInteger(); private final Map<String, PendingRequestInfo> sentRequestMap = new LinkedHashMap<>(); private final Map<String, CompletableFuture<?>> receivedRequestMap = new LinkedHashMap<>(); //... //constructor area /** * @param out - a consumer that transmits messages to the remote service * @param localEndpoint - the local service implementation * @param exceptionHandler - an exception handler that should never return null. */ public RemoteEndpoint(MessageConsumer out, Endpoint localEndpoint, Function<Throwable, ResponseError> exceptionHandler) { if (out == null) throw new NullPointerException(\"out\"); if (localEndpoint == null) throw new NullPointerException(\"localEndpoint\"); if (exceptionHandler == null) throw new NullPointerException(\"exceptionHandler\"); this.out = out; this.localEndpoint = localEndpoint; this.exceptionHandler = exceptionHandler; } /** * @param out - a consumer that transmits messages to the remote service * @param localEndpoint - the local service implementation */ public RemoteEndpoint(MessageConsumer out, Endpoint localEndpoint) { this(out, localEndpoint, DEFAULT_EXCEPTION_HANDLER); } // ... other methods ... }","title":"Extra - RemoteEndpoint class"},{"location":"Development docs/Language Server/ClientServerHandshake/#builder-operations-workflow","text":"Create an instance of MessageJsonHandler (using internal method createJsonHandler() ): MessageJsonHandler works as a wrapper around Gson that includes configuration necessary for the RPC communication. Using its routine getSupportedMethods() the MessageJsonHandler will be populated with the list of all RPC supported methods both for client and server. With the defined MessageJsonHandler object, the builder invoke the createRemoteEndpoint() method. Here a new Endpoint is created with a defined MessageConsumer and Endpoint defined.","title":"Builder operations workflow"},{"location":"Development docs/Language Server/ClientServerHandshake/#note","text":"ServiceEndpoints.toEndpoint(localServices) is a static method defined by the ServiceEndpoint class and it wraps a given object with service annotations behind an Endpoint interface. /** * Wraps a collection of objects with service annotations behind an {@link Endpoint} interface. * * @return the wrapped service endpoint */ public static Endpoint toEndpoint(Collection<Object> serviceObjects) { return new GenericEndpoint(serviceObjects); } Next is necessary define the remoteProxy object that will be used by the LS to set the LanguageClient class (that will be used to inspect methods available that the server will use to send messages to the server) In order to define the RemoteProxy a static method provided by ServiceEndpoints will be used. This routine wraps a given Endpoint in the given service interface. java /** * Wraps a given {@link Endpoint} in the given service interface. * * @return the wrapped service object */ @SuppressWarnings(\"unchecked\") public static <T> T toServiceObject(Endpoint endpoint, Class<T> interface_) { Class<?>[] interfArray = new Class[]{interface_, Endpoint.class}; EndpointProxy invocationHandler = new EndpointProxy(endpoint, interface_); return (T) Proxy.newProxyInstance(interface_.getClassLoader(), interfArray, invocationHandler); } 4. After that is necessary define the MessageConsumer that reads from an input stream and parses messages from JSON 5. Finally is necessary define a new ExecutorService or use one already defined and sent to the createLauncher() method 6. After all these steps, a new anonymous class for Launcher<T> is created and are also defined methods that the new class need to override from the Launcher interface: @SuppressWarnings(\"unchecked\") public Launcher<T> create() { //... return new Launcher<T> () { @Override public Future<Void> startListening() { return ConcurrentMessageProcessor.startProcessing(reader, messageConsumer, execService); } @Override public T getRemoteProxy() { return remoteProxy; } @Override public RemoteEndpoint getRemoteEndpoint() { return remoteEndpoint; } }; }","title":"Note"},{"location":"Development docs/Language Server/ClientServerHandshake/#startlistening-startprocessing-method","text":"The startListening() method is defined by each class that implements the Launcher interface, and also anonymous classes. This method invoke the static startProcessing() method defined by the class ConcurrentMessageProcessor , defined into the package org.eclipse.lsp4j.jsonrpc.json . The class ConcurrentMessageProcessor implements the Runnable interface and its static method startProcessing return a Future<Void> that will be resolved when the started thread is terminated: /** * Start a thread that listens for messages in the message producer and forwards them to the message consumer. * * @param messageProducer - produces messages, e.g. by reading from an input channel * @param messageConsumer - processes messages and potentially forwards them to other consumers * @param executorService - the thread is started using this service * @return a future that is resolved when the started thread is terminated, e.g. by closing a stream */ public static Future<Void> startProcessing(MessageProducer messageProducer, MessageConsumer messageConsumer, ExecutorService executorService) { ConcurrentMessageProcessor reader = new ConcurrentMessageProcessor(messageProducer, messageConsumer); final Future<?> result = executorService.submit(reader); return new Future<Void>() { @Override public Void get() throws InterruptedException, ExecutionException { return (Void) result.get(); } @Override public Void get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException { return (Void) result.get(timeout, unit); } @Override public boolean isDone() { return result.isDone(); } @Override public boolean cancel(boolean mayInterruptIfRunning) { if (mayInterruptIfRunning && messageProducer instanceof Closeable) { try { ((Closeable) messageProducer).close(); } catch (IOException e) { throw new RuntimeException(e); } } return result.cancel(mayInterruptIfRunning); } @Override public boolean isCancelled() { return result.isCancelled(); } };","title":"startListening() &amp; startProcessing() method"},{"location":"Development docs/Language Server/VS-Code Extension/","text":"VS-Code extension for the Language Server","title":"VS-Code extension for the Language Server"},{"location":"Development docs/Language Server/VS-Code Extension/#vs-code-extension-for-the-language-server","text":"","title":"VS-Code extension for the Language Server"},{"location":"Development docs/Language Server/langserver-spec/","text":"Langage Server specification ... WORK IN PROGRESS ... /** * The show message notification is sent from a server to a client to ask * the client to display a particular message in the user interface. */ @JsonNotification(\"window/showMessage\") void showMessage(MessageParams messageParams); /** * The show message request is sent from a server to a client to ask the * client to display a particular message in the user interface. In addition * to the show message notification the request allows to pass actions and * to wait for an answer from the client. */ @JsonRequest(\"window/showMessageRequest\") CompletableFuture<MessageActionItem> showMessageRequest(ShowMessageRequestParams requestParams);","title":"Langage Server specification"},{"location":"Development docs/Language Server/langserver-spec/#langage-server-specification","text":"","title":"Langage Server specification"},{"location":"Development docs/Language Server/langserver-spec/#work-in-progress","text":"/** * The show message notification is sent from a server to a client to ask * the client to display a particular message in the user interface. */ @JsonNotification(\"window/showMessage\") void showMessage(MessageParams messageParams); /** * The show message request is sent from a server to a client to ask the * client to display a particular message in the user interface. In addition * to the show message notification the request allows to pass actions and * to wait for an answer from the client. */ @JsonRequest(\"window/showMessageRequest\") CompletableFuture<MessageActionItem> showMessageRequest(ShowMessageRequestParams requestParams);","title":"... WORK IN PROGRESS ..."},{"location":"Development docs/Language Server/lsp4j/","text":"Implement a Language Server in java The definition of Language server introduced by Microsoft can revolutionize the way to define common operations made available for an editor. Several time ago when an editor was created, it was necessary implement all the business logic to provide some activities very common in an code editor as keyword highlighting, auto-hint and auto-complete and so on. But what happens if I define only one language server and provide a way to use it as a server that send/receive messages from multiple editor (clients)? Communication through JSON-RPC 2.0 The communication between editor clients and language server are made using JSON-RPC protocol but it is also possible add HTTP level transportation on top. In this way is possible consume the LS as a remote server with URL endpoints to establish the communication and also provide some health checks. The communication is bidirectional, this means that both client and server can send and receive messages from/to the opposite component. More information about standards, specification and provided implementations for several languages in the link Implement a LS in Java The language server specification provided by Microsoft consists in a collection of interfaces with specific responsibility for each kind of operation that the LS implementation want support or not. In order to define the capabilities of our LS is necessary implements the interfaces provided by the standard. On Java side there is a very useful collection of interfaces for LSP standard: LSP4J . More in depth implementation Implementation notes In this workaround, client and sever components are defined in the same Java project setting up using Maven. The output of our personal LS server will be a jar that will be invoked from the shell. First of all add LSP4J/LSP4J JSON-RPC as maven dependency into the pom.xml : <!-- https://mvnrepository.com/artifact/org.eclipse.lsp4j/org.eclipse.lsp4j.jsonrpc --> <dependency> <groupId>org.eclipse.lsp4j</groupId> <artifactId>org.eclipse.lsp4j.jsonrpc</artifactId> <version>0.4.1</version> </dependency> <!-- https://mvnrepository.com/artifact/org.eclipse.lsp4j/org.eclipse.lsp4j --> <dependency> <groupId>org.eclipse.lsp4j</groupId> <artifactId>org.eclipse.lsp4j</artifactId> <version>0.4.1</version> </dependency> After that we need to define two interfaces that extends the LanguageServer and the LanguageClient. Of course we need to define its implementation classes to be able to made some action as client or as server. Note The interfaces LanguageServer and LanguageClient implement the Endpoint interface. In this way the underhood communication layer from clients to the server is already managed. @JsonSegment(\"server\") public interface IMyLanguageServer extends LanguageServer {} @JsonSegment(\"client\") public interface IMyLanguageClient extends LanguageClient { public void start(LanguageServer serverLauncher); } Note The annotation @JsonSegment are provided by LSP4J and are used to define the internal routing of our methods. For example for the client start method the URI will be /client/start. The server concrete implementation: public class MyLanguageServerImpl implements IMyLanguageServer { private final List<LanguageClient> clients = new CopyOnWriteArrayList<>(); private TextDocumentService textService; private WorkspaceService workspaceService; public MyLanguageServerImpl() { //initialize textDocumentService and workspaceService } public CompletableFuture<InitializeResult> initialize(InitializeParams params) { // TODO Auto-generated method stub return null; } public CompletableFuture<Object> shutdown() { // TODO Auto-generated method stub return null; } public void exit() { // TODO Auto-generated method stub } public TextDocumentService getTextDocumentService() { // TODO Auto-generated method stub return null; } public WorkspaceService getWorkspaceService() { // TODO Auto-generated method stub return null; } public Runnable setRemoteProxy(LanguageClient languageClient) { this.clients.add(languageClient); return () -> this.clients.remove(languageClient); } public List<LanguageClient> getClientList() { return clients; } } Note In this overview is not specified the entire implementation of the language server implementation, because the focus is on the set of operations to define the LS and be able to listen for incoming communications with other clients. A more in depth overview about LS implementation will be added in the future in a separate post. And the client concrete implementation: public class MyLanguageClientImpl implements IMyLanguageClient { public void telemetryEvent(Object object) { // TODO Auto-generated method stub } public void publishDiagnostics(PublishDiagnosticsParams diagnostics) { // TODO Auto-generated method stub } public void showMessage(MessageParams messageParams) { // TODO Auto-generated method stub } public CompletableFuture<MessageActionItem> showMessageRequest( ShowMessageRequestParams requestParams) { // TODO Auto-generated method stub return null; } public void logMessage(MessageParams message) { // TODO Auto-generated method stub } @Override public void start(LanguageServer serverLauncher) { System.out.println(\"CLIENT SEND REQUEST TO THE SERVER\"); serverLauncher.initialize(null); } } Note - 1 In a common scenario where the language server is used by a well defined client is not necessary implement both client and server, but just the server. The underline communication with JSON-RPC will be able to realize the sharing messages from the two actors. Note - 2 The start() method defined in the client interface will be used in the main and using the communication channel provided by the server we can send a request to the server. After that as last step is necessary define the main method and turn all on in order to implement the client/server comunication: public class App { static Logger logger = LogManager.getLogger(Main.class); public static void main(String[] args) throws InterruptedException, ExecutionException { logger.warn(\"Start debugging\"); startServer(System.in, System.out); } public static void startServer(InputStream in, OutputStream out) throws InterruptedException, ExecutionException { MyLanguageServerImpl myServer = new MyLanguageServerImpl(); Launcher<LanguageClient> l = LSPLauncher.createServerLauncher(myServer, in, out); Future<?> startListening = l.startListening(); myServer.setRemoteProxy(l.getRemoteProxy()); startListening.get(); } } So the key method is createServerLauncher() . Their purpose is to create the endpoint for the opposite site (that need to know who want call it) and manage the stream channel communication (in this case will be the standard input and standard output). startListining() allow the server to be in listening mode for incoming connection from a client. setRemoteProxy() is invoked to register the language client retrieved by the proxy (Launcher ) into the internal list that will be used by the other services defined in order to be compliant with the LSP standard (i.e. TextDocumentService concrete implementation class). When everything is defined correctly is possible deploy the jar with the maven command mvn package and in order to test that the server doesn't go in some exceptions is possible run it from the terminal, into the target folder where the jar is generated: java -jar mylangserver-0.0.1-jar-with-dependencies.jar A possible output could be this one: 09:35:17.647 [main] WARN com.test.lsp.Main - Start debugging _ Note If the client already exsist and it is compliant to the LSP definition is not necessary start the server in a standalone mode but is necessary provide a plugin that define the path of the language server and provide the client implementation of the LSP. In the next posts will be analyzed the definition of client implementation realized with: - Visual Studio Code extension plugin - Eclipse IDE plugin - Eclipse Che The cool things (and obliviously the success of the LSP approach) is that for three different client there is only ONE implementation of the LS.","title":"Implement a Language Server in java"},{"location":"Development docs/Language Server/lsp4j/#implement-a-language-server-in-java","text":"The definition of Language server introduced by Microsoft can revolutionize the way to define common operations made available for an editor. Several time ago when an editor was created, it was necessary implement all the business logic to provide some activities very common in an code editor as keyword highlighting, auto-hint and auto-complete and so on. But what happens if I define only one language server and provide a way to use it as a server that send/receive messages from multiple editor (clients)?","title":"Implement a Language Server in java"},{"location":"Development docs/Language Server/lsp4j/#communication-through-json-rpc-20","text":"The communication between editor clients and language server are made using JSON-RPC protocol but it is also possible add HTTP level transportation on top. In this way is possible consume the LS as a remote server with URL endpoints to establish the communication and also provide some health checks. The communication is bidirectional, this means that both client and server can send and receive messages from/to the opposite component. More information about standards, specification and provided implementations for several languages in the link","title":"Communication through JSON-RPC 2.0"},{"location":"Development docs/Language Server/lsp4j/#implement-a-ls-in-java","text":"The language server specification provided by Microsoft consists in a collection of interfaces with specific responsibility for each kind of operation that the LS implementation want support or not. In order to define the capabilities of our LS is necessary implements the interfaces provided by the standard. On Java side there is a very useful collection of interfaces for LSP standard: LSP4J .","title":"Implement a LS in Java"},{"location":"Development docs/Language Server/lsp4j/#more-in-depth-implementation","text":"","title":"More in depth implementation"},{"location":"Development docs/Language Server/lsp4j/#implementation-notes","text":"In this workaround, client and sever components are defined in the same Java project setting up using Maven. The output of our personal LS server will be a jar that will be invoked from the shell. First of all add LSP4J/LSP4J JSON-RPC as maven dependency into the pom.xml : <!-- https://mvnrepository.com/artifact/org.eclipse.lsp4j/org.eclipse.lsp4j.jsonrpc --> <dependency> <groupId>org.eclipse.lsp4j</groupId> <artifactId>org.eclipse.lsp4j.jsonrpc</artifactId> <version>0.4.1</version> </dependency> <!-- https://mvnrepository.com/artifact/org.eclipse.lsp4j/org.eclipse.lsp4j --> <dependency> <groupId>org.eclipse.lsp4j</groupId> <artifactId>org.eclipse.lsp4j</artifactId> <version>0.4.1</version> </dependency> After that we need to define two interfaces that extends the LanguageServer and the LanguageClient. Of course we need to define its implementation classes to be able to made some action as client or as server.","title":"Implementation notes"},{"location":"Development docs/Language Server/lsp4j/#note","text":"The interfaces LanguageServer and LanguageClient implement the Endpoint interface. In this way the underhood communication layer from clients to the server is already managed. @JsonSegment(\"server\") public interface IMyLanguageServer extends LanguageServer {} @JsonSegment(\"client\") public interface IMyLanguageClient extends LanguageClient { public void start(LanguageServer serverLauncher); }","title":"Note"},{"location":"Development docs/Language Server/lsp4j/#note_1","text":"The annotation @JsonSegment are provided by LSP4J and are used to define the internal routing of our methods. For example for the client start method the URI will be /client/start. The server concrete implementation: public class MyLanguageServerImpl implements IMyLanguageServer { private final List<LanguageClient> clients = new CopyOnWriteArrayList<>(); private TextDocumentService textService; private WorkspaceService workspaceService; public MyLanguageServerImpl() { //initialize textDocumentService and workspaceService } public CompletableFuture<InitializeResult> initialize(InitializeParams params) { // TODO Auto-generated method stub return null; } public CompletableFuture<Object> shutdown() { // TODO Auto-generated method stub return null; } public void exit() { // TODO Auto-generated method stub } public TextDocumentService getTextDocumentService() { // TODO Auto-generated method stub return null; } public WorkspaceService getWorkspaceService() { // TODO Auto-generated method stub return null; } public Runnable setRemoteProxy(LanguageClient languageClient) { this.clients.add(languageClient); return () -> this.clients.remove(languageClient); } public List<LanguageClient> getClientList() { return clients; } }","title":"Note"},{"location":"Development docs/Language Server/lsp4j/#note_2","text":"In this overview is not specified the entire implementation of the language server implementation, because the focus is on the set of operations to define the LS and be able to listen for incoming communications with other clients. A more in depth overview about LS implementation will be added in the future in a separate post. And the client concrete implementation: public class MyLanguageClientImpl implements IMyLanguageClient { public void telemetryEvent(Object object) { // TODO Auto-generated method stub } public void publishDiagnostics(PublishDiagnosticsParams diagnostics) { // TODO Auto-generated method stub } public void showMessage(MessageParams messageParams) { // TODO Auto-generated method stub } public CompletableFuture<MessageActionItem> showMessageRequest( ShowMessageRequestParams requestParams) { // TODO Auto-generated method stub return null; } public void logMessage(MessageParams message) { // TODO Auto-generated method stub } @Override public void start(LanguageServer serverLauncher) { System.out.println(\"CLIENT SEND REQUEST TO THE SERVER\"); serverLauncher.initialize(null); } }","title":"Note"},{"location":"Development docs/Language Server/lsp4j/#note-1","text":"In a common scenario where the language server is used by a well defined client is not necessary implement both client and server, but just the server. The underline communication with JSON-RPC will be able to realize the sharing messages from the two actors.","title":"Note - 1"},{"location":"Development docs/Language Server/lsp4j/#note-2","text":"The start() method defined in the client interface will be used in the main and using the communication channel provided by the server we can send a request to the server. After that as last step is necessary define the main method and turn all on in order to implement the client/server comunication: public class App { static Logger logger = LogManager.getLogger(Main.class); public static void main(String[] args) throws InterruptedException, ExecutionException { logger.warn(\"Start debugging\"); startServer(System.in, System.out); } public static void startServer(InputStream in, OutputStream out) throws InterruptedException, ExecutionException { MyLanguageServerImpl myServer = new MyLanguageServerImpl(); Launcher<LanguageClient> l = LSPLauncher.createServerLauncher(myServer, in, out); Future<?> startListening = l.startListening(); myServer.setRemoteProxy(l.getRemoteProxy()); startListening.get(); } } So the key method is createServerLauncher() . Their purpose is to create the endpoint for the opposite site (that need to know who want call it) and manage the stream channel communication (in this case will be the standard input and standard output). startListining() allow the server to be in listening mode for incoming connection from a client. setRemoteProxy() is invoked to register the language client retrieved by the proxy (Launcher ) into the internal list that will be used by the other services defined in order to be compliant with the LSP standard (i.e. TextDocumentService concrete implementation class). When everything is defined correctly is possible deploy the jar with the maven command mvn package and in order to test that the server doesn't go in some exceptions is possible run it from the terminal, into the target folder where the jar is generated: java -jar mylangserver-0.0.1-jar-with-dependencies.jar A possible output could be this one: 09:35:17.647 [main] WARN com.test.lsp.Main - Start debugging _","title":"Note - 2"},{"location":"Development docs/Language Server/lsp4j/#note_3","text":"If the client already exsist and it is compliant to the LSP definition is not necessary start the server in a standalone mode but is necessary provide a plugin that define the path of the language server and provide the client implementation of the LSP. In the next posts will be analyzed the definition of client implementation realized with: - Visual Studio Code extension plugin - Eclipse IDE plugin - Eclipse Che The cool things (and obliviously the success of the LSP approach) is that for three different client there is only ONE implementation of the LS.","title":"Note"},{"location":"Development docs/Language Server/why-lsp/","text":"Why a LSP? Large number of IDEs, why I need to rewrite a compiler several times for each different languages/environment? a good principle could be (good enough approach): define generic editing features use a language specific configuration using an external service is possible provide a correct separation of responsibilities: editor developers are focused on the editor part (provide plugin, new functionality) language developers are focused on the language support to provide for all the IDEs any developer - any language - any tool How it works on hi-level the ide will connect to the language server the language server is a separate component (usually a background process) the communication between two components is realized using JSON RPC messages LSP Java implementation = LSP4J Java implementation of LSP JSON messages communication, result wrapped in a object with methods and so on APIs for implement LSP (interfaces) client/server endpoints Workthough example ||define this snippet with gist client invocation: JSON RPC: Manage a LSP use a defined SDK and implement the specifications here the available SDKs: https://microsoft.github.io/language-server-protocol/implementors/sdks/ here the specification rules: https://microsoft.github.io/language-server-protocol/specification use an already defined LSP servers to integrate in the IDE more info at https://microsoft.github.io/language-server-protocol/implementors/servers/","title":"Why a LSP?"},{"location":"Development docs/Language Server/why-lsp/#why-a-lsp","text":"Large number of IDEs, why I need to rewrite a compiler several times for each different languages/environment? a good principle could be (good enough approach): define generic editing features use a language specific configuration using an external service is possible provide a correct separation of responsibilities: editor developers are focused on the editor part (provide plugin, new functionality) language developers are focused on the language support to provide for all the IDEs any developer - any language - any tool","title":"Why a LSP?"},{"location":"Development docs/Language Server/why-lsp/#how-it-works-on-hi-level","text":"the ide will connect to the language server the language server is a separate component (usually a background process) the communication between two components is realized using JSON RPC messages","title":"How it works on hi-level"},{"location":"Development docs/Language Server/why-lsp/#lsp-java-implementation-lsp4j","text":"Java implementation of LSP JSON messages communication, result wrapped in a object with methods and so on APIs for implement LSP (interfaces) client/server endpoints","title":"LSP Java implementation = LSP4J"},{"location":"Development docs/Language Server/why-lsp/#workthough-example","text":"||define this snippet with gist client invocation: JSON RPC:","title":"Workthough example"},{"location":"Development docs/Language Server/why-lsp/#manage-a-lsp","text":"use a defined SDK and implement the specifications here the available SDKs: https://microsoft.github.io/language-server-protocol/implementors/sdks/ here the specification rules: https://microsoft.github.io/language-server-protocol/specification use an already defined LSP servers to integrate in the IDE more info at https://microsoft.github.io/language-server-protocol/implementors/servers/","title":"Manage a LSP"},{"location":"Development docs/Maven/maven-multi-project/","text":"Apache Maven dev note Create a multi-module project Create the main project starting from a folder and convert it into a maven project as defined from the defined inside. define the packaging as 'pom' and define the two project as module: xml <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>it.andzac.dvlp</groupId> <artifactId>mproject</artifactId> <version>0.0.1-SNAPSHOT</version> <packaging>pom</packaging> <modules> <module>engine</module> <module>rpc</module> </modules> </project> Create the two sub-module project and define in their pom the relation with the defined parent (for the subproject is not necessary define group and version because it is inherit from the parent): xml <project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"> <modelVersion>4.0.0</modelVersion> <parent> <groupId>it.andzac.dvlp</groupId> <artifactId>mproject</artifactId> <version>0.0.1-SNAPSHOT</version> </parent> <artifactId>engine</artifactId> <name>engine</name> <!-- ...... --> </project> 4. If a module need a dependency with another module of the main (parent) project, is necessary add it as dependency in the dependencies section (In the example below engine need a dependency of rpc module and the pom listed below is relative to engine): xml <!-- ...... --> <dependency> <groupId>com.ca.lsp</groupId> <artifactId>rpc</artifactId> <version>0.0.1-SNAPSHOT</version> </dependency> In order to make the jar executable (with the necessary dependencies inside it) is necessary define the build section: xml <!-- ...... --> <build> <plugins> <plugin> <artifactId>maven-assembly-plugin</artifactId> <version>3.1.0</version> <configuration> <archive> <manifest> <addClasspath>true</addClasspath> <mainClass>it.andzac.mproject.engine.App</mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef>jar-with-dependencies</descriptorRef> </descriptorRefs> </configuration> <executions> <execution> <id>make-assembly</id> <phase>package</phase> <goals> <goal>single</goal> </goals> </execution> </executions> </plugin> </plugins> </build> Note In the <archive> tag is defined the main class that will be invoked when the jar will be launched. Now is possible invoke the maven command to create the jar that can be executed directly from the shell: sh mvn clean install -DskipTests mvn package java -jar engine-0.0.1-SNAPSHOT-jar-with-dependencies.jar","title":"Apache Maven dev note"},{"location":"Development docs/Maven/maven-multi-project/#apache-maven-dev-note","text":"","title":"Apache Maven dev note"},{"location":"Development docs/Maven/maven-multi-project/#create-a-multi-module-project","text":"Create the main project starting from a folder and convert it into a maven project as defined from the defined inside. define the packaging as 'pom' and define the two project as module: xml <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>it.andzac.dvlp</groupId> <artifactId>mproject</artifactId> <version>0.0.1-SNAPSHOT</version> <packaging>pom</packaging> <modules> <module>engine</module> <module>rpc</module> </modules> </project> Create the two sub-module project and define in their pom the relation with the defined parent (for the subproject is not necessary define group and version because it is inherit from the parent): xml <project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"> <modelVersion>4.0.0</modelVersion> <parent> <groupId>it.andzac.dvlp</groupId> <artifactId>mproject</artifactId> <version>0.0.1-SNAPSHOT</version> </parent> <artifactId>engine</artifactId> <name>engine</name> <!-- ...... --> </project> 4. If a module need a dependency with another module of the main (parent) project, is necessary add it as dependency in the dependencies section (In the example below engine need a dependency of rpc module and the pom listed below is relative to engine): xml <!-- ...... --> <dependency> <groupId>com.ca.lsp</groupId> <artifactId>rpc</artifactId> <version>0.0.1-SNAPSHOT</version> </dependency> In order to make the jar executable (with the necessary dependencies inside it) is necessary define the build section: xml <!-- ...... --> <build> <plugins> <plugin> <artifactId>maven-assembly-plugin</artifactId> <version>3.1.0</version> <configuration> <archive> <manifest> <addClasspath>true</addClasspath> <mainClass>it.andzac.mproject.engine.App</mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef>jar-with-dependencies</descriptorRef> </descriptorRefs> </configuration> <executions> <execution> <id>make-assembly</id> <phase>package</phase> <goals> <goal>single</goal> </goals> </execution> </executions> </plugin> </plugins> </build>","title":"Create a multi-module project"},{"location":"Development docs/Maven/maven-multi-project/#note","text":"In the <archive> tag is defined the main class that will be invoked when the jar will be launched. Now is possible invoke the maven command to create the jar that can be executed directly from the shell: sh mvn clean install -DskipTests mvn package java -jar engine-0.0.1-SNAPSHOT-jar-with-dependencies.jar","title":"Note"},{"location":"Development docs/Serverless/openFaaS/","text":"OpenFaas for Serverless The Serverless era Faas (Function as a Service) is a modern methodology that split the service layer in a set of functions that are available ONLY when necessary. This approach allow to design the serverless architecture that are becoming very popular in the cloud. When an user start a transaction to buy a Coca cola at the cash a new instance of the payment function service is created and it will be destroyed after the end of the transaction. This allow to save resource, save money and test it in isolation mode using containers. The basic concept is: \"I will pay the cloud provider only when the function is invoked by a client\" . If no one are using my function I don't need to pay a flat rent, consuming resources and money for something that now is not necessary. OpenFaaS as a serverless solution Basically doesn't matter if there are some servers and where are they located. The developer should only define functions. The container orchestrator (Kubernetes for example) will take care of where the function should be deployed and how much hardware resources it needs. So OpenFaaS is simply a way to put function in Kubernetes defined as small containers that the orchestrator needs to manage. Some notes about Faas architecture: API gateway is the place where new functions are defined. Plus: user can obtain functions from cloud repository or push new ones in the same way as for docker store! A watchdog component is defined in all docker containers that host the function and it take care about all the operations necessary to deploy the container, pick the necessary hw resources and so on Prometheus collects stats about our stack with a dashboard that reports all the useful information (number of times that the function is invoked, number of instances and so on) Implement FaaS with Minikube Minikube is a Kubernetes distribution that allow to create a simple nodes cluster on the fly. Below a workthrough tutorial. OpenFaas with Minikube State of art Serverless technologies are the new trend that several companies has chosen. Actually there are many implementations of the serverless principles, just some examples Apache OpenWisk Fission Kubeless Serverless Nuclio A big challenge is try to define a common standard language for events that fire the functions defined in the API gateway independently from the framework chosen for realize the serverless approach . More about CloudEvents project at https://cloudevents.io/ Bonus track: Vue.js In order to write functions in a serverless approach, JavaScript is the way and Vue.js is particularly useful to design functions. Vue.js conference Other lectures and references How to start with OpenFaaS Homemade Serverless OpenFaaS official site OS18 Prague - OpenFaaS Save money with Serverless","title":"OpenFaas for Serverless"},{"location":"Development docs/Serverless/openFaaS/#openfaas-for-serverless","text":"","title":"OpenFaas for Serverless"},{"location":"Development docs/Serverless/openFaaS/#the-serverless-era","text":"Faas (Function as a Service) is a modern methodology that split the service layer in a set of functions that are available ONLY when necessary. This approach allow to design the serverless architecture that are becoming very popular in the cloud. When an user start a transaction to buy a Coca cola at the cash a new instance of the payment function service is created and it will be destroyed after the end of the transaction. This allow to save resource, save money and test it in isolation mode using containers. The basic concept is: \"I will pay the cloud provider only when the function is invoked by a client\" . If no one are using my function I don't need to pay a flat rent, consuming resources and money for something that now is not necessary.","title":"The Serverless era"},{"location":"Development docs/Serverless/openFaaS/#openfaas-as-a-serverless-solution","text":"Basically doesn't matter if there are some servers and where are they located. The developer should only define functions. The container orchestrator (Kubernetes for example) will take care of where the function should be deployed and how much hardware resources it needs. So OpenFaaS is simply a way to put function in Kubernetes defined as small containers that the orchestrator needs to manage. Some notes about Faas architecture: API gateway is the place where new functions are defined. Plus: user can obtain functions from cloud repository or push new ones in the same way as for docker store! A watchdog component is defined in all docker containers that host the function and it take care about all the operations necessary to deploy the container, pick the necessary hw resources and so on Prometheus collects stats about our stack with a dashboard that reports all the useful information (number of times that the function is invoked, number of instances and so on)","title":"OpenFaaS as a serverless solution"},{"location":"Development docs/Serverless/openFaaS/#implement-faas-with-minikube","text":"Minikube is a Kubernetes distribution that allow to create a simple nodes cluster on the fly. Below a workthrough tutorial. OpenFaas with Minikube","title":"Implement FaaS with Minikube"},{"location":"Development docs/Serverless/openFaaS/#state-of-art","text":"Serverless technologies are the new trend that several companies has chosen. Actually there are many implementations of the serverless principles, just some examples Apache OpenWisk Fission Kubeless Serverless Nuclio A big challenge is try to define a common standard language for events that fire the functions defined in the API gateway independently from the framework chosen for realize the serverless approach . More about CloudEvents project at https://cloudevents.io/","title":"State of art"},{"location":"Development docs/Serverless/openFaaS/#bonus-track-vuejs","text":"In order to write functions in a serverless approach, JavaScript is the way and Vue.js is particularly useful to design functions. Vue.js conference","title":"Bonus track: Vue.js"},{"location":"Development docs/Serverless/openFaaS/#other-lectures-and-references","text":"How to start with OpenFaaS Homemade Serverless OpenFaaS official site OS18 Prague - OpenFaaS Save money with Serverless","title":"Other lectures and references"},{"location":"Development docs/TypeScript6/typescript/","text":"Typescript function greeter(person: string) { return \"Hello, \" + person; } let user = \"Jane User\"; document.body.innerHTML = greeter(user); to compile the code type in a terminal: tsc greeter.ts Typescript add a static check on top of js compiler can guess with values are valid for some variables defined","title":"Typescript"},{"location":"Development docs/TypeScript6/typescript/#typescript","text":"function greeter(person: string) { return \"Hello, \" + person; } let user = \"Jane User\"; document.body.innerHTML = greeter(user); to compile the code type in a terminal: tsc greeter.ts Typescript add a static check on top of js compiler can guess with values are valid for some variables defined","title":"Typescript"},{"location":"Development docs/mkdocs/Project layout/","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Development docs/mkdocs/commands/","text":"Commands Headers # = Section title (h1) ## = Paragraph title (h2) ### = Sub-paragraph title (h2) Lists mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Note When a text that introduce a list leave an empty row from the text and the list Tables First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell First Header | Second Header | Third Header ------------ | ------------- | ------------ Content Cell | Content Cell | Content Cell Content Cell | Content Cell | Content Cell Note In order to past multiline into mkdocs source code pages is necessary use fences. More information at http://mkdocs.readthedocs.io/en/0.10/user-guide/writing-your-docs/","title":"Commands"},{"location":"Development docs/mkdocs/commands/#commands","text":"","title":"Commands"},{"location":"Development docs/mkdocs/commands/#headers","text":"# = Section title (h1) ## = Paragraph title (h2) ### = Sub-paragraph title (h2)","title":"Headers"},{"location":"Development docs/mkdocs/commands/#lists","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Lists"},{"location":"Development docs/mkdocs/commands/#note","text":"When a text that introduce a list leave an empty row from the text and the list","title":"Note"},{"location":"Development docs/mkdocs/commands/#tables","text":"First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell First Header | Second Header | Third Header ------------ | ------------- | ------------ Content Cell | Content Cell | Content Cell Content Cell | Content Cell | Content Cell","title":"Tables"},{"location":"Development docs/mkdocs/commands/#note_1","text":"In order to past multiline into mkdocs source code pages is necessary use fences. More information at http://mkdocs.readthedocs.io/en/0.10/user-guide/writing-your-docs/","title":"Note"},{"location":"Development docs/mkdocs/theming/","text":"Modify a defined theme Access to the folder where the template is defined (below in according with the default configuration): /usr/lib/python2.7/site-packages/theme_name A theme package is defined with this files structure: -rw-r--r--. 1 root root 88 13 mag 15.51 404.html drwxr-xr-x. 5 root root 4096 13 mag 15.51 assets -rw-r--r--. 1 root root 8857 26 mag 23.30 base.html -rw-r--r--. 1 root root 0 13 mag 15.51 __init__.py -rw-r--r--. 1 root root 145 13 mag 15.51 __init__.pyc -rw-r--r--. 1 root root 26 13 mag 15.51 main.html -rw-r--r--. 1 root root 2678 13 mag 15.51 theme_name.yml drwxr-xr-x. 4 root root 4096 26 mag 23.33 partials The partials folder contains the page elements as header, footer and other stuffs -rw-r--r--. 1 root root 2427 26 mag 23.33 footer.html -rw-r--r--. 1 root root 2049 13 mag 15.51 header.html -rw-r--r--. 1 root root 284 13 mag 15.51 hero.html drwxr-xr-x. 2 root root 4096 13 mag 15.51 integrations drwxr-xr-x. 2 root root 4096 13 mag 15.51 language -rw-r--r--. 1 root root 421 13 mag 15.51 language.html -rw-r--r--. 1 root root 907 13 mag 15.51 nav.html -rw-r--r--. 1 root root 1975 13 mag 15.51 nav-item.html -rw-r--r--. 1 root root 1115 13 mag 15.51 search.html -rw-r--r--. 1 root root 390 13 mag 15.51 social.html -rw-r--r--. 1 root root 853 13 mag 15.51 source.html -rw-r--r--. 1 root root 369 13 mag 15.51 tabs.html -rw-r--r--. 1 root root 1161 13 mag 15.51 tabs-item.html -rw-r--r--. 1 root root 1294 13 mag 15.51 toc.html -rw-r--r--. 1 root root 387 13 mag 15.51 toc-item.html Each html file can be customized with a set of template variables defined with Jinja-2 syntax.","title":"Theming"},{"location":"Development docs/mkdocs/theming/#modify-a-defined-theme","text":"Access to the folder where the template is defined (below in according with the default configuration): /usr/lib/python2.7/site-packages/theme_name A theme package is defined with this files structure: -rw-r--r--. 1 root root 88 13 mag 15.51 404.html drwxr-xr-x. 5 root root 4096 13 mag 15.51 assets -rw-r--r--. 1 root root 8857 26 mag 23.30 base.html -rw-r--r--. 1 root root 0 13 mag 15.51 __init__.py -rw-r--r--. 1 root root 145 13 mag 15.51 __init__.pyc -rw-r--r--. 1 root root 26 13 mag 15.51 main.html -rw-r--r--. 1 root root 2678 13 mag 15.51 theme_name.yml drwxr-xr-x. 4 root root 4096 26 mag 23.33 partials The partials folder contains the page elements as header, footer and other stuffs -rw-r--r--. 1 root root 2427 26 mag 23.33 footer.html -rw-r--r--. 1 root root 2049 13 mag 15.51 header.html -rw-r--r--. 1 root root 284 13 mag 15.51 hero.html drwxr-xr-x. 2 root root 4096 13 mag 15.51 integrations drwxr-xr-x. 2 root root 4096 13 mag 15.51 language -rw-r--r--. 1 root root 421 13 mag 15.51 language.html -rw-r--r--. 1 root root 907 13 mag 15.51 nav.html -rw-r--r--. 1 root root 1975 13 mag 15.51 nav-item.html -rw-r--r--. 1 root root 1115 13 mag 15.51 search.html -rw-r--r--. 1 root root 390 13 mag 15.51 social.html -rw-r--r--. 1 root root 853 13 mag 15.51 source.html -rw-r--r--. 1 root root 369 13 mag 15.51 tabs.html -rw-r--r--. 1 root root 1161 13 mag 15.51 tabs-item.html -rw-r--r--. 1 root root 1294 13 mag 15.51 toc.html -rw-r--r--. 1 root root 387 13 mag 15.51 toc-item.html Each html file can be customized with a set of template variables defined with Jinja-2 syntax.","title":"Modify a defined theme"},{"location":"English lessons/newTerms/","text":"Terms Meaning (ITA) crowd mass of people (folla) waving move with vibration (agitare) toddler childs that learn to walk (bambino piccolo) despite althrough (nonostante) zaniness strange/funny behaviour (spenseriatezza) bother scary (preoccupazione) queasy disgusting (nauseare) shyness timidity (timidezza) reveling in your own success (godere del proprio successo) undergone loss out (subita) jostling fight (spingersi) beaming of joy (raggianti di gioia) roar scream (ruggito) bond binding (legarsi) more greater than themselves (qualcosa pi\u00f9 grande di loro) cumbersome (ingombrante) meaningful significant (significativo) brotherhood fraternity (fraternanza) resilience elaxsticity (elasticit\u00e0) ruthless cruel (spietatezza) quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuzbaz quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuz baz qux quux quuzbaz quux quuz baz qux quux quuz","title":"newTerms"},{"location":"English lessons/verbs/","text":"Verbs tense: put the verb in the present/past/future aspect: the action is completed or not Simple --> evento nella sua interezza Continuos --> evento in corso di svolgimento Perfect --> evento concluso di cui si analizza la frequenza/quantit\u00e0 Perfect continuos --> evento in corso in tutto il tempo precedente il momento indicato Present time present simple: look(s) usual actions events that happens always feelings, brain activities This ops takes a lot of time I usually go to home at 5:00 p.m. I don't understand Do you belive me? present continuos: be + \"ing\" suffix actions that happens while speaking I'm watching the television. Quiet! I'm thinking. We are having a terrible day! present perfect: have + pp event and action that are just done to indicate how much times something happens to indicate since how much time something happens with how much/how many (quantities) I've finished my homework now. I've already joined in the meeting. I've lived in Prague for two months. All the people that have looked at this code say that it's ok How many emails have you written? present perfect continuos: have + been + \"ing\" suffix as the present perfect it describe an action in the past that is not specified with \"How long\" and with since/for plus it not indicates that the action is done or not should be used for temporal expressions as lately, for a long time and so on I've been reading books lately. (Sto leggendo libri recentemente - non indicato se \u00e8 conclusa) It's been raining all day. (E' tutto il giorno che piove) How long have you been executing the test? I've been cooking for many hours. NOTE: Present Perfect vs Past Simple when is indicated that an action is done IN THE PAST is necessary use the past simple: I phoned Michael yesterday ~~I have phoned Michael yesterday~~ NOTE: Present Perfect vs Present Perfect Continuos when the attention is dedicated on the result of an action it's used the present perferct: I've bought a lot of chocolate for the cake when the attention is targered on the action itself it's used the present perfect continuos: I've been writing emails for a couple of hours","title":"Verbs"},{"location":"English lessons/verbs/#verbs","text":"tense: put the verb in the present/past/future aspect: the action is completed or not Simple --> evento nella sua interezza Continuos --> evento in corso di svolgimento Perfect --> evento concluso di cui si analizza la frequenza/quantit\u00e0 Perfect continuos --> evento in corso in tutto il tempo precedente il momento indicato","title":"Verbs"},{"location":"English lessons/verbs/#present-time","text":"present simple: look(s) usual actions events that happens always feelings, brain activities This ops takes a lot of time I usually go to home at 5:00 p.m. I don't understand Do you belive me? present continuos: be + \"ing\" suffix actions that happens while speaking I'm watching the television. Quiet! I'm thinking. We are having a terrible day! present perfect: have + pp event and action that are just done to indicate how much times something happens to indicate since how much time something happens with how much/how many (quantities) I've finished my homework now. I've already joined in the meeting. I've lived in Prague for two months. All the people that have looked at this code say that it's ok How many emails have you written? present perfect continuos: have + been + \"ing\" suffix as the present perfect it describe an action in the past that is not specified with \"How long\" and with since/for plus it not indicates that the action is done or not should be used for temporal expressions as lately, for a long time and so on I've been reading books lately. (Sto leggendo libri recentemente - non indicato se \u00e8 conclusa) It's been raining all day. (E' tutto il giorno che piove) How long have you been executing the test? I've been cooking for many hours.","title":"Present time"},{"location":"English lessons/verbs/#note-present-perfect-vs-past-simple","text":"when is indicated that an action is done IN THE PAST is necessary use the past simple: I phoned Michael yesterday ~~I have phoned Michael yesterday~~","title":"NOTE: Present Perfect vs Past Simple"},{"location":"English lessons/verbs/#note-present-perfect-vs-present-perfect-continuos","text":"when the attention is dedicated on the result of an action it's used the present perferct: I've bought a lot of chocolate for the cake when the attention is targered on the action itself it's used the present perfect continuos: I've been writing emails for a couple of hours","title":"NOTE: Present Perfect vs Present Perfect Continuos"},{"location":"Life in Prague/Useful information/","text":"From airport to city center","title":"Useful information"},{"location":"Life in Prague/Useful information/#from-airport-to-city-center","text":"","title":"From airport to city center"},{"location":"OS2018/Egkatastasis/egkatastasis/","text":"OpenSUSE Egkatastasis Testing packages installation with Docker A docker container offers the opportunity to define a custom image, a little vm shiped with all the required software and configuration file that are necessary to run the application inside of it. When a new container is released for production (that could be the docker store or a private image repository for the client project) it is necessary garantee two things: The application that runs inside the container works fine, don't exceed the space defined, basically is consistent with the requirement. The enviroment where the application runs is well defined, there aren't any broken files or missing packages. The open source project Egkatastasis, created by OpenSUSE engineer Panagiotis Georgiadis try to garantee the point #2 for the OpenSUSE container. Hi-level architecture Egkatastasis is basically an enviroment that can be used for test each packages in a separate container. The container used for testing the packages is always the same but is created/destroyed on the fly for each package defined in a specific list that the bash script process read. The log produced during the process for each container is retrieved by Filebeat, enriched with additional information with Logstash and indexed by ElasticSearch, with a Kibana dashboard available to see stats and analyze/filter logs for each package tested. With this methodology a docker image with several packages could be splitted in many mini-containers, verify if everything works fine. Also it could be useful to test a specific package with several linux distribution, in the future could be realized a web interface with integrated analysis tools as Kibana. A simple walkthrough checkout the repository from github move into the docker folder and launch the script that need as argument the package that will be installed into the docker containers ./testit.sh vim during the process a new container and its relative logs are generated and managed by the log stack. When the testing operation is completed the container is no longer exists but its logs file are saved locally and it are available also on the Kibana dashboard on web. References OS18 Talk - youtube video","title":"OpenSUSE Egkatastasis"},{"location":"OS2018/Egkatastasis/egkatastasis/#opensuse-egkatastasis","text":"","title":"OpenSUSE Egkatastasis"},{"location":"OS2018/Egkatastasis/egkatastasis/#testing-packages-installation-with-docker","text":"A docker container offers the opportunity to define a custom image, a little vm shiped with all the required software and configuration file that are necessary to run the application inside of it. When a new container is released for production (that could be the docker store or a private image repository for the client project) it is necessary garantee two things: The application that runs inside the container works fine, don't exceed the space defined, basically is consistent with the requirement. The enviroment where the application runs is well defined, there aren't any broken files or missing packages. The open source project Egkatastasis, created by OpenSUSE engineer Panagiotis Georgiadis try to garantee the point #2 for the OpenSUSE container.","title":"Testing packages installation with Docker"},{"location":"OS2018/Egkatastasis/egkatastasis/#hi-level-architecture","text":"Egkatastasis is basically an enviroment that can be used for test each packages in a separate container. The container used for testing the packages is always the same but is created/destroyed on the fly for each package defined in a specific list that the bash script process read. The log produced during the process for each container is retrieved by Filebeat, enriched with additional information with Logstash and indexed by ElasticSearch, with a Kibana dashboard available to see stats and analyze/filter logs for each package tested. With this methodology a docker image with several packages could be splitted in many mini-containers, verify if everything works fine. Also it could be useful to test a specific package with several linux distribution, in the future could be realized a web interface with integrated analysis tools as Kibana.","title":"Hi-level architecture"},{"location":"OS2018/Egkatastasis/egkatastasis/#a-simple-walkthrough","text":"checkout the repository from github move into the docker folder and launch the script that need as argument the package that will be installed into the docker containers ./testit.sh vim during the process a new container and its relative logs are generated and managed by the log stack. When the testing operation is completed the container is no longer exists but its logs file are saved locally and it are available also on the Kibana dashboard on web.","title":"A simple walkthrough"},{"location":"OS2018/Egkatastasis/egkatastasis/#references","text":"OS18 Talk - youtube video","title":"References"},{"location":"OS2018/Logging Container/syslog-ng/","text":"Logging container with syslog-ng Manage log files centrally In a context with several different container, it is very hard manage a lot of logs file. Define a single component to collect all the log messages allow to: Manage logs in a easy way because there are only one source for all the logs produced by containers if containers crash the logs are still available and no stat data are affected from the disaster ops Main components of the syslog-ng solution collector: from different sources (files, sockets, pipe) processor: allow to parse amount of data with parser of different types (db parser, python, etc..) apply anonymization technique to protect sensitive information defined in the logs transform the output in a specific format to give at the other infrastructures enrich data with specific additional information filtering avoid not necessary logs using comparisions, wildcard, regular expr and function concatenate all the possibilities with boolean operators storage flat files sent through HTTP(s) big data management (ES, MongoDB, Handhoop and so on) ## Configuration -- attach example of code and trivial user case ## Syslog-ng + Docker - Syslog-ng project started in 1998 - different implementation - with docker is possible define the container that will be used as central server -- approfondire docker infrastructure & container logs journal logs??","title":"Logging container with syslog-ng"},{"location":"OS2018/Logging Container/syslog-ng/#logging-container-with-syslog-ng","text":"","title":"Logging container with syslog-ng"},{"location":"OS2018/Logging Container/syslog-ng/#manage-log-files-centrally","text":"In a context with several different container, it is very hard manage a lot of logs file. Define a single component to collect all the log messages allow to: Manage logs in a easy way because there are only one source for all the logs produced by containers if containers crash the logs are still available and no stat data are affected from the disaster ops","title":"Manage log files centrally"},{"location":"OS2018/Logging Container/syslog-ng/#main-components-of-the-syslog-ng-solution","text":"collector: from different sources (files, sockets, pipe) processor: allow to parse amount of data with parser of different types (db parser, python, etc..) apply anonymization technique to protect sensitive information defined in the logs transform the output in a specific format to give at the other infrastructures enrich data with specific additional information filtering avoid not necessary logs using comparisions, wildcard, regular expr and function concatenate all the possibilities with boolean operators storage flat files sent through HTTP(s) big data management (ES, MongoDB, Handhoop and so on) ## Configuration -- attach example of code and trivial user case ## Syslog-ng + Docker - Syslog-ng project started in 1998 - different implementation - with docker is possible define the container that will be used as central server -- approfondire docker infrastructure & container logs journal logs??","title":"Main components of the syslog-ng solution"}]}